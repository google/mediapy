<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>mediapy API documentation</title>
<meta name="description" content="`mediapy`: Read/write/show images and videos in an IPython/Jupyter notebook â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Package <code>mediapy</code></h1>
</header>
<section id="section-intro">
<p><code><a title="mediapy" href="#mediapy">mediapy</a></code>: Read/write/show images and videos in an IPython/Jupyter notebook.</p>
<p>See the <a href="https://github.com/google/mediapy/blob/main/html/mediapy/index.html"><strong>online
documentation</strong></a>.</p>
<p>See the examples in the notebook
<a href="https://github.com/google/mediapy/blob/main/mediapy_examples.ipynb">mediapy_examples.ipynb</a>.</p>
<p>Better yet, <a href="https://colab.research.google.com/github/google/mediapy/blob/main/mediapy_examples.ipynb"><strong>open the notebook in
Colab</strong></a>.</p>
<h2 id="image-examples">Image examples</h2>
<p>Display an image (2D or 3D <code>numpy</code> array):</p>
<pre><code class="language-python">  checkerboard = np.kron([[0, 1] * 16, [1, 0] * 16] * 16, np.ones((4, 4)))
  show_image(checkerboard)
</code></pre>
<p>Read and display an image (either local or from the Web):</p>
<pre><code class="language-python">  IMAGE = 'https://github.com/hhoppe/data/raw/main/image.png'
  show_image(read_image(IMAGE))
</code></pre>
<p>Read and display an image from a local file:</p>
<pre><code class="language-python">  !wget -q -O /tmp/burano.png {IMAGE}
  show_image(read_image('/tmp/burano.png'))
</code></pre>
<p>Show titled images side-by-side:</p>
<pre><code class="language-python">  images = {
      'original': checkerboard,
      'darkened': checkerboard * 0.7,
      'random': np.random.rand(32, 32, 3),
  }
  show_images(images, vmin=0.0, vmax=1.0, border=True, height=64)
</code></pre>
<h2 id="video-examples">Video examples</h2>
<p>Display a video (an iterable of images, e.g., a 3D or 4D array):</p>
<pre><code class="language-python">  video = moving_circle((100, 100), num_images=10)
  show_video(video)
</code></pre>
<p>Show the video frames side-by-side:</p>
<pre><code class="language-python">  show_images(video, columns=6, border=True, height=64)
</code></pre>
<p>Show the frames with their indices:</p>
<pre><code class="language-python">  show_images({f'{i}': image for i, image in enumerate(video)}, width=32)
</code></pre>
<p>Read and display a video (either local or from the Web):</p>
<pre><code class="language-python">  VIDEO = 'https://github.com/hhoppe/data/raw/main/video.mp4'
  show_video(read_video(VIDEO))
</code></pre>
<p>Create and display a looping two-frame GIF video:</p>
<pre><code class="language-python">  image1 = resize_image(np.random.rand(10, 10, 3), (50, 50))
  show_video([image1, image1 * 0.8], fps=2, codec='gif')
</code></pre>
<p>Darken a video frame-by-frame:</p>
<pre><code class="language-python">  output_path = '/tmp/out.mp4'
  with VideoReader(VIDEO) as r:
    darken_image = lambda image: to_float01(image) * 0.5
    with VideoWriter(output_path, shape=r.shape, fps=r.fps, bps=r.bps) as w:
      for image in r:
        w.add_image(darken_image(image))
  show_video(read_video(output_path), height=90)
</code></pre>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># Lint as: python3

# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &#34;AS IS&#34; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
&#34;&#34;&#34;`mediapy`: Read/write/show images and videos in an IPython/Jupyter notebook.

See the [**online
documentation**](https://github.com/google/mediapy/blob/main/html/mediapy/index.html).

See the examples in the notebook
[mediapy_examples.ipynb](https://github.com/google/mediapy/blob/main/mediapy_examples.ipynb).

Better yet, [**open the notebook in
Colab**](https://colab.research.google.com/github/google/mediapy/blob/main/mediapy_examples.ipynb).

## Image examples

Display an image (2D or 3D `numpy` array):
```python
  checkerboard = np.kron([[0, 1] * 16, [1, 0] * 16] * 16, np.ones((4, 4)))
  show_image(checkerboard)
```

Read and display an image (either local or from the Web):
```python
  IMAGE = &#39;https://github.com/hhoppe/data/raw/main/image.png&#39;
  show_image(read_image(IMAGE))
```

Read and display an image from a local file:
```python
  !wget -q -O /tmp/burano.png {IMAGE}
  show_image(read_image(&#39;/tmp/burano.png&#39;))
```

Show titled images side-by-side:
```python
  images = {
      &#39;original&#39;: checkerboard,
      &#39;darkened&#39;: checkerboard * 0.7,
      &#39;random&#39;: np.random.rand(32, 32, 3),
  }
  show_images(images, vmin=0.0, vmax=1.0, border=True, height=64)
```

## Video examples

Display a video (an iterable of images, e.g., a 3D or 4D array):
```python
  video = moving_circle((100, 100), num_images=10)
  show_video(video)
```

Show the video frames side-by-side:
```python
  show_images(video, columns=6, border=True, height=64)
```

Show the frames with their indices:
```python
  show_images({f&#39;{i}&#39;: image for i, image in enumerate(video)}, width=32)
```

Read and display a video (either local or from the Web):
```python
  VIDEO = &#39;https://github.com/hhoppe/data/raw/main/video.mp4&#39;
  show_video(read_video(VIDEO))
```

Create and display a looping two-frame GIF video:
```python
  image1 = resize_image(np.random.rand(10, 10, 3), (50, 50))
  show_video([image1, image1 * 0.8], fps=2, codec=&#39;gif&#39;)
```

Darken a video frame-by-frame:
```python
  output_path = &#39;/tmp/out.mp4&#39;
  with VideoReader(VIDEO) as r:
    darken_image = lambda image: to_float01(image) * 0.5
    with VideoWriter(output_path, shape=r.shape, fps=r.fps, bps=r.bps) as w:
      for image in r:
        w.add_image(darken_image(image))
  show_video(read_video(output_path), height=90)
```
&#34;&#34;&#34;

import base64
import collections.abc
import contextlib
import functools
import importlib
import io
import itertools
import math
import numbers
import os
import pathlib
import re
import shlex
import shutil
import stat
import subprocess
import sys
import tempfile
import typing
from typing import Any, Callable, ContextManager, Generator, Iterable
from typing import Iterator, List, Mapping, Optional, Sequence
from typing import Tuple, TypeVar, Union
import urllib.request

import IPython.display
import matplotlib.pyplot as plt
import numpy as np
import PIL.Image
import PIL.ImageOps

_IPYTHON_HTML_SIZE_LIMIT = 20_000_000
_T = TypeVar(&#39;_T&#39;)

# https://github.com/python/mypy/issues/5667
_Path = Union[&#39;os.PathLike[str]&#39;]
_StrOrPath = Union[str, _Path]

# Name or path for ffmpeg program required for reading/writing of video.  It may
# be an absolute path or a filename within a directory of os.environ[&#39;PATH&#39;].
FFMPEG_NAME: str = &#39;ffmpeg&#39;

## Miscellaneous.


def _open(path: _StrOrPath, *args: Any, **kwargs: Any) -&gt; ContextManager[Any]:
  &#34;&#34;&#34;Opens the file; this is a hook for the built-in open().&#34;&#34;&#34;
  return open(path, *args, **kwargs)


def _path_is_local(path: _StrOrPath) -&gt; bool:
  &#34;&#34;&#34;Returns True if the path is in the filesystem accessible by &#39;ffmpeg&#39;.&#34;&#34;&#34;
  del path
  return True


def _search_for_ffmpeg_path() -&gt; Optional[str]:
  &#34;&#34;&#34;Returns a path to the ffmpeg program, or None if not found.&#34;&#34;&#34;
  filename = shutil.which(FFMPEG_NAME)
  if filename:
    return filename
  return None


def _print_err(*args: str, **kwargs: Any) -&gt; None:
  &#34;&#34;&#34;Prints arguments to stderr immediately.&#34;&#34;&#34;
  kwargs = {**dict(file=sys.stderr, flush=True), **kwargs}
  print(*args, **kwargs)


def _chunked(iterable: Iterable[_T],
             n: Optional[int] = None) -&gt; Iterator[Tuple[_T, ...]]:
  &#34;&#34;&#34;Returns elements collected as tuples of length at most &#39;n&#39; if not None.&#34;&#34;&#34;

  def take(n: int, iterable: Iterable[_T]) -&gt; Tuple[_T, ...]:
    return tuple(itertools.islice(iterable, n))

  return iter(functools.partial(take, n, iter(iterable)), ())


def peek_first(iterator: Iterable[_T]) -&gt; Tuple[_T, Iterable[_T]]:
  &#34;&#34;&#34;Given an iterator, returns first element and re-initialized iterator.

  &gt;&gt;&gt; first_image, images = peek_first(moving_circle())

  Args:
    iterator: An input iterator or iterable.

  Returns:
    A tuple (first_element, iterator_reinitialized) containing:
      first_element: The first element of the input.
      iterator_reinitialized: A clone of the original iterator/iterable.
  &#34;&#34;&#34;
  # Inspired from https://stackoverflow.com/a/12059829/1190077
  peeker, iterator_reinitialized = itertools.tee(iterator)
  first = next(peeker)
  return first, iterator_reinitialized


def _check_2d_shape(shape: Tuple[int, int]) -&gt; None:
  &#34;&#34;&#34;Checks that &#39;shape&#39; is of the form (height, width) with two integers.&#34;&#34;&#34;
  if len(shape) != 2:
    raise ValueError(f&#39;Shape {shape} is not of the form (height, width).&#39;)
  if not all(isinstance(i, numbers.Integral) for i in shape):
    raise ValueError(f&#39;Shape {shape} contains non-integers.&#39;)


def run(args: Union[str, Sequence[str]]) -&gt; None:
  &#34;&#34;&#34;Executes command, printing output from stdout and stderr.

  Args:
    args: Command to execute, which can be either a string or a sequence of word
      strings, as in `subprocess.run()`.  If `args` is a string, the shell is
      invoked to interpret it.

  Raises:
    RuntimeError: If the command&#39;s exit code is nonzero.
  &#34;&#34;&#34;
  proc = subprocess.run(
      args,
      shell=isinstance(args, str),
      stdout=subprocess.PIPE,
      stderr=subprocess.STDOUT,
      check=False,
      universal_newlines=True)
  print(proc.stdout, end=&#39;&#39;, flush=True)
  if proc.returncode:
    raise RuntimeError(
        f&#34;Command &#39;{proc.args}&#39; failed with code {proc.returncode}.&#34;)


def set_output_height(num_pixels: int) -&gt; None:
  &#34;&#34;&#34;Overrides the height of the current output cell, if using Colab.&#34;&#34;&#34;
  try:
    # We want to fail gracefully for non-Colab IPython notebooks.
    output = importlib.import_module(&#39;google.colab.output&#39;)
    s = f&#39;google.colab.output.setIframeHeight(&#34;{num_pixels}px&#34;)&#39;
    output.eval_js(s)  # type: ignore
  except ModuleNotFoundError:
    pass


def set_max_output_height(num_pixels: int) -&gt; None:
  &#34;&#34;&#34;Sets the maximum height of the current output cell, if using Colab.&#34;&#34;&#34;
  try:
    # We want to fail gracefully for non-Colab IPython notebooks.
    output = importlib.import_module(&#39;google.colab.output&#39;)
    s = (&#39;google.colab.output.setIframeHeight(&#39;
         f&#39;0, true, {{maxHeight: {num_pixels}}})&#39;)
    output.eval_js(s)  # type: ignore
  except ModuleNotFoundError:
    pass


## Type conversions.


def _as_valid_media_type(dtype: Any) -&gt; Any:
  &#34;&#34;&#34;Returns validated media data type.&#34;&#34;&#34;
  dtype = np.dtype(dtype)
  if not issubclass(dtype.type, (np.unsignedinteger, np.floating)):
    raise ValueError(
        f&#39;Type {dtype} is not a valid media data type (uint or float).&#39;)
  return dtype


def _as_valid_media_array(x: Iterable[Any]) -&gt; np.ndarray:
  &#34;&#34;&#34;Converts to ndarray (if not already), and checks validity of data type.&#34;&#34;&#34;
  a = np.asarray(x)
  if a.dtype == bool:
    a = a.astype(np.uint8) * np.iinfo(np.uint8).max
  _as_valid_media_type(a.dtype)
  return a


def to_type(a: Any, dtype: Any) -&gt; np.ndarray:
  &#34;&#34;&#34;Returns media array converted to specified type.

  A &#34;media array&#34; is one in which the dtype is either a floating-point type
  (np.float32 or np.float64) or an unsigned integer type.  The array values are
  assumed to lie in the range [0.0, 1.0] for floating-point values, and in the
  full range for unsigned integers, e.g. [0, 255] for np.uint8.  Conversion
  between integers and floats maps uint(0) to 0.0 and uint(MAX) to 1.0.  The
  input array may also be of type bool, whereby True maps to uint(MAX) or 1.0.
  The values are scaled and clamped as appropriate during type conversions.

  Args:
    a: Input array-like object (of type floating-point, unsigned int, or bool).
    dtype: Desired output type (floating-point or unsigned int).

  Returns:
    Array `a` if it is already of the specified dtype, else a converted array.
  &#34;&#34;&#34;
  a = np.asarray(a)
  dtype = _as_valid_media_type(dtype)
  if a.dtype != bool:
    _as_valid_media_type(a.dtype)  # Verify that &#39;a&#39; has a valid dtype.
  if a.dtype == bool:
    result = a.astype(dtype)
    if issubclass(dtype.type, np.unsignedinteger):
      result = result * dtype.type(np.iinfo(dtype).max)
  elif a.dtype == dtype:
    result = a
  elif issubclass(dtype.type, np.unsignedinteger):
    if issubclass(a.dtype.type, np.unsignedinteger):
      src_max = np.iinfo(a.dtype).max
    else:
      a = np.clip(a, 0.0, 1.0)
      src_max = 1.0
    dst_max = np.iinfo(dtype).max
    if dst_max &lt;= np.iinfo(np.uint16).max:
      result = (a * np.float32(dst_max / src_max) + 0.5).astype(dtype)
    elif dst_max &lt;= np.iinfo(np.uint32).max:
      result = (a.astype(np.float64) * (dst_max / src_max) + 0.5).astype(dtype)
    else:
      # https://stackoverflow.com/a/66306123/
      a = a.astype(np.float64) * (dst_max / src_max) + 0.5
      dst = np.atleast_1d(a)
      values_too_large = dst &gt;= np.float64(dst_max)
      dst = dst.astype(dtype)
      dst[values_too_large] = dst_max
      result = dst if a.ndim &gt; 0 else dst[0]
  else:
    assert issubclass(dtype.type, np.floating)
    result = a.astype(dtype)
    if issubclass(a.dtype.type, np.unsignedinteger):
      result = result / dtype.type(np.iinfo(a.dtype).max)
  return result


def to_float01(a: Any, dtype: Any = np.float32) -&gt; np.ndarray:
  &#34;&#34;&#34;If array has unsigned integers, rescales them to the range [0.0, 1.0].

  Args:
    a: Input array.
    dtype: Desired floating-point type if rescaling occurs.

  Returns:
    A new array of dtype values in the range [0.0, 1.0] if the input array `a`
    contains unsigned integers; otherwise, array `a` is returned unchanged.

  Scaling is such that uint(0) maps to 0.0 and uint(MAX) maps to 1.0.
  &#34;&#34;&#34;
  dtype = np.dtype(dtype)
  if not issubclass(dtype.type, np.floating):
    raise ValueError(f&#39;Type {dtype} is not floating-point.&#39;)
  a = np.asarray(a)
  if issubclass(a.dtype.type, np.floating):
    return a
  return to_type(a, dtype)


def to_uint(a: Any, dtype: Any) -&gt; np.ndarray:
  &#34;&#34;&#34;Returns array converted to unsigned-integer dtype; see `to_type`.&#34;&#34;&#34;
  dtype = np.dtype(dtype)
  if not issubclass(dtype.type, np.unsignedinteger):
    raise ValueError(f&#39;Type {dtype} is not an unsigned integer.&#39;)
  return to_type(a, dtype)


def to_uint8(a: Any) -&gt; np.ndarray:
  &#34;&#34;&#34;Returns array converted to uint8 values; see `to_type`.&#34;&#34;&#34;
  return to_type(a, np.uint8)


## Functions to generate example image and video data.


def color_ramp(shape: Tuple[int, int] = (64, 64), *,
               dtype: Any = np.float32) -&gt; np.ndarray:
  &#34;&#34;&#34;Returns an image of a red-green color gradient.

  This is useful for quick experimentation and testing.  See also
  `moving_circle` to generate a sample video.

  Args:
    shape: 2D spatial dimensions (height, width) of generated image.
    dtype: Type (uint or floating) of resulting pixel values.
  &#34;&#34;&#34;
  _check_2d_shape(shape)
  dtype = _as_valid_media_type(dtype)
  yx = (np.moveaxis(np.indices(shape), 0, -1) + 0.5) / shape
  image = np.insert(yx, 2, 0.0, axis=-1)
  return to_type(image, dtype)


def moving_circle(shape: Tuple[int, int] = (256, 256),
                  num_images: int = 10,
                  *,
                  dtype: Any = np.float32) -&gt; np.ndarray:
  &#34;&#34;&#34;Returns a video of a circle moving in front of a color ramp.

  This is useful for quick experimentation and testing.  See also `color_ramp`
  to generate a sample image.

  &gt;&gt;&gt; show_video(moving_circle((480, 640), 60))

  Args:
    shape: 2D spatial dimensions (height, width) of generated video.
    num_images: Number of video frames.
    dtype: Type (uint or floating) of resulting pixel values.
  &#34;&#34;&#34;
  _check_2d_shape(shape)
  dtype = np.dtype(dtype)

  def generate_image(image_index: int) -&gt; np.ndarray:
    &#34;&#34;&#34;Returns a video frame image.&#34;&#34;&#34;
    image = color_ramp(shape, dtype=dtype)
    yx = np.moveaxis(np.indices(shape), 0, -1)
    center = (shape[0] * 0.6, shape[1] * (image_index + 0.5) / num_images)
    radius_squared = (min(shape) * 0.1)**2
    inside = np.sum((yx - center)**2, axis=-1) &lt; radius_squared
    white_circle_color = (1.0, 1.0, 1.0)
    if issubclass(dtype.type, np.unsignedinteger):
      white_circle_color = to_uint([white_circle_color], dtype)[0]
    image[inside] = white_circle_color
    return image

  return np.array([generate_image(i) for i in range(num_images)])


## Color-space conversions.

# Same matrix values as in two sources:
# https://github.com/scikit-image/scikit-image/blob/master/skimage/color/colorconv.py#L377
# https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/ops/image_ops_impl.py#L2754
_YUV_FROM_RGB_MATRIX = np.array(
    [[0.299, -0.14714119, 0.61497538], [0.587, -0.28886916, -0.51496512],
     [0.114, 0.43601035, -0.10001026]],
    dtype=np.float32)
_RGB_FROM_YUV_MATRIX = np.linalg.inv(_YUV_FROM_RGB_MATRIX)
_YUV_CHROMA_OFFSET = np.array([0.0, 0.5, 0.5], dtype=np.float32)


def yuv_from_rgb(rgb: Any) -&gt; np.ndarray:
  &#34;&#34;&#34;Returns the RGB image/video mapped to YUV [0,1] color space.

  Note that the &#34;YUV&#34; color space used by video compressors is actually YCbCr!

  Args:
    rgb: Input image in sRGB space.
  &#34;&#34;&#34;
  rgb = to_float01(rgb)
  if rgb.shape[-1] != 3:
    raise ValueError(f&#39;The last dimension in {rgb.shape} is not 3.&#39;)
  return np.matmul(rgb, _YUV_FROM_RGB_MATRIX) + _YUV_CHROMA_OFFSET


def rgb_from_yuv(yuv: Any) -&gt; np.ndarray:
  &#34;&#34;&#34;Returns the YUV image/video mapped to RGB [0,1] color space.&#34;&#34;&#34;
  yuv = to_float01(yuv)
  if yuv.shape[-1] != 3:
    raise ValueError(f&#39;The last dimension in {yuv.shape} is not 3.&#39;)
  return np.matmul(yuv - _YUV_CHROMA_OFFSET, _RGB_FROM_YUV_MATRIX)


# Same matrix values as in
# https://github.com/scikit-image/scikit-image/blob/master/skimage/color/colorconv.py#L1654
# and https://en.wikipedia.org/wiki/YUV#Studio_swing_for_BT.601
_YCBCR_FROM_RGB_MATRIX = np.array(
    [[65.481, 128.553, 24.966], [-37.797, -74.203, 112.0],
     [112.0, -93.786, -18.214]],
    dtype=np.float32).transpose()
_RGB_FROM_YCBCR_MATRIX = np.linalg.inv(_YCBCR_FROM_RGB_MATRIX)
_YCBCR_OFFSET = np.array([16.0, 128.0, 128.0], dtype=np.float32)
# Note that _YCBCR_FROM_RGB_MATRIX =~ _YUV_FROM_RGB_MATRIX * [219, 256, 182];
# https://en.wikipedia.org/wiki/YUV: &#34;Y&#39; values are conventionally shifted and
# scaled to the range [16, 235] (referred to as studio swing or &#39;TV levels&#39;)&#34;;
# &#34;studio range of 16-240 for U and V&#34;.  (Where does value 182 come from?)


def ycbcr_from_rgb(rgb: Any) -&gt; np.ndarray:
  &#34;&#34;&#34;Returns the RGB image/video mapped to YCbCr [0,1] color space.

  The YCbCr color space is the one called &#34;YUV&#34; by video compressors.

  Args:
    rgb: Input image in sRGB space.
  &#34;&#34;&#34;
  rgb = to_float01(rgb)
  if rgb.shape[-1] != 3:
    raise ValueError(f&#39;The last dimension in {rgb.shape} is not 3.&#39;)
  return (np.matmul(rgb, _YCBCR_FROM_RGB_MATRIX) + _YCBCR_OFFSET) / 255.0


def rgb_from_ycbcr(ycbcr: Any) -&gt; np.ndarray:
  &#34;&#34;&#34;Returns the YCbCr image/video mapped to RGB [0,1] color space.&#34;&#34;&#34;
  ycbcr = to_float01(ycbcr)
  if ycbcr.shape[-1] != 3:
    raise ValueError(f&#39;The last dimension in {ycbcr.shape} is not 3.&#39;)
  return np.matmul(ycbcr * 255.0 - _YCBCR_OFFSET, _RGB_FROM_YCBCR_MATRIX)


## Image processing.


def _pil_image(image: Any, mode: Optional[str] = None) -&gt; PIL.Image.Image:
  &#34;&#34;&#34;Returns a PIL image given a numpy matrix (either uint8 or float [0,1]).&#34;&#34;&#34;
  image = _as_valid_media_array(image)
  if image.ndim not in (2, 3):
    raise ValueError(f&#39;Image shape {image.shape} is neither 2D nor 3D.&#39;)
  return PIL.Image.fromarray(image, mode=mode)


def resize_image(image: Any, shape: Tuple[int, int]) -&gt; np.ndarray:
  &#34;&#34;&#34;Resizes image to specified spatial dimensions using a Lanczos filter.

  Args:
    image: Array-like 2D or 3D object, where dtype is uint or floating-point.
    shape: 2D spatial dimensions (height, width) of output image.

  Returns:
    A resampled image whose spatial dimensions match `shape`.
  &#34;&#34;&#34;
  image = _as_valid_media_array(image)
  if image.ndim not in (2, 3):
    raise ValueError(f&#39;Image shape {image.shape} is neither 2D nor 3D.&#39;)
  _check_2d_shape(shape)

  # A PIL image can be multichannel only if it has 3 or 4 uint8 channels,
  # and it can be resized only if it is uint8 or float32.
  supported_single_channel = ((issubclass(image.dtype.type, np.floating) or
                               image.dtype == np.uint8) and image.ndim == 2)
  supported_multichannel = (
      image.dtype == np.uint8 and image.ndim == 3 and image.shape[2] in (3, 4))
  if supported_single_channel or supported_multichannel:
    return np.array(
        _pil_image(image).resize(shape[::-1], resample=PIL.Image.LANCZOS),
        dtype=image.dtype)
  if image.ndim == 2:
    # We convert to floating-poing for resizing and convert back.
    return to_type(resize_image(to_float01(image), shape), image.dtype)
  # We resize each image channel individually.
  return np.dstack(
      [resize_image(channel, shape) for channel in np.moveaxis(image, -1, 0)])


## Video processing.


def resize_video(video: Iterable[np.ndarray], shape: Tuple[int,
                                                           int]) -&gt; np.ndarray:
  &#34;&#34;&#34;Resizes `video` to specified spatial dimensions using a Lanczos filter.

  Args:
    video: Iterable of images.
    shape: 2D spatial dimensions (height, width) of output video.

  Returns:
    A resampled video whose spatial dimensions match `shape`.
  &#34;&#34;&#34;
  _check_2d_shape(shape)
  return np.array([resize_image(image, shape) for image in video])


## General I/O.


def _is_url(path_or_url: _StrOrPath) -&gt; bool:
  return isinstance(path_or_url, str) and path_or_url.startswith(
      (&#39;http://&#39;, &#39;https://&#39;, &#39;file://&#39;))


def read_contents(path_or_url: _StrOrPath) -&gt; bytes:
  &#34;&#34;&#34;Returns the contents of the file specified by either a path or URL.&#34;&#34;&#34;
  data: bytes
  if _is_url(path_or_url):
    assert isinstance(path_or_url, str)
    with urllib.request.urlopen(path_or_url) as response:
      data = response.read()
  else:
    with _open(path_or_url, &#39;rb&#39;) as f:
      data = f.read()
  return data


@contextlib.contextmanager
def read_via_local_file(path_or_url: _StrOrPath) -&gt; Generator[str, None, None]:
  &#34;&#34;&#34;Context to copy a remote file locally to read from it.

  Args:
    path_or_url: File, which may be remote.

  Yields:
    The name of a local file which may be a copy of a remote file.
  &#34;&#34;&#34;
  if _is_url(path_or_url) or not _path_is_local(path_or_url):
    suffix = pathlib.Path(path_or_url).suffix
    with tempfile.TemporaryDirectory() as directory_name:
      tmp_path = pathlib.Path(directory_name) / f&#39;file{suffix}&#39;
      tmp_path.write_bytes(read_contents(path_or_url))
      yield str(tmp_path)
  else:
    yield str(path_or_url)


@contextlib.contextmanager
def write_via_local_file(path: _StrOrPath) -&gt; Generator[str, None, None]:
  &#34;&#34;&#34;Context to write a temporary local file and subsequently copy it remotely.

  Args:
    path: File, which may be remote.

  Yields:
    The name of a local file which may be subsequently copied remotely.
  &#34;&#34;&#34;
  if _path_is_local(path):
    yield str(path)
  else:
    suffix = pathlib.Path(path).suffix
    with tempfile.TemporaryDirectory() as directory_name:
      tmp_path = pathlib.Path(directory_name) / f&#39;file{suffix}&#39;
      yield str(tmp_path)
      with _open(path, mode=&#39;wb&#39;) as f:
        f.write(tmp_path.read_bytes())


class _ShowSave:
  &#34;&#34;&#34;Saves outputs from `show_image` and `show_video` into files.&#34;&#34;&#34;
  dir: Optional[_StrOrPath] = None

  @contextlib.contextmanager
  def to_dir(self, path: _StrOrPath) -&gt; Generator[None, None, None]:
    &#34;&#34;&#34;Temporarily sets output directory for show_image() and show_video().&#34;&#34;&#34;
    previous_dir = self.dir
    try:
      self.dir = path
      yield
    finally:
      self.dir = previous_dir


show_save = _ShowSave()
&#34;&#34;&#34;Functionality to save all titled output from `show_*()` calls into files.

If `dir` attribute of `show_save` is not None, all titled images and videos
displayed by `show_image`, `show_images`, `show_video`, and `show_videos` are
also saved as files within the specified directory.

The context `to_dir(directory_name)` temporarily assigns the
`dir` attribute:

&gt;&gt;&gt; with show_save.to_dir(&#39;/tmp&#39;):
...   show_image(image, title=&#39;image1&#39;)  # Creates /tmp/image1.png.
...   show_video(video, title=&#39;video2&#39;)  # Creates /tmp/video2.mp4.

Attributes:
  dir: Directory into which to save titled images and videos, or None.
&#34;&#34;&#34;

## Image I/O.


def read_image(path_or_url: _StrOrPath, *, dtype: Any = np.uint8) -&gt; np.ndarray:
  &#34;&#34;&#34;Returns an image read from a file path or URL.&#34;&#34;&#34;
  dtype = np.dtype(dtype)
  data = read_contents(path_or_url)
  return decompress_image(data, dtype)


def write_image(path: _StrOrPath, image: np.ndarray, **kwargs: Any) -&gt; None:
  &#34;&#34;&#34;Writes an image to a file.

  Encoding is performed using `PIL`, which supports `uint8` images with 1, 3,
  or 4 channels and `uint16` images with a single channel.

  Args:
    path: Path of output file.
    image: Array-like object.  If its type is float, it is converted to np.uint8
      using `to_uint8` (thus clamping to the input to the range [0.0, 1.0]).
      Otherwise it must be np.uint8 or np.uint16.
    **kwargs: Additional parameters for `PIL.Image.save()`.
  &#34;&#34;&#34;
  if issubclass(image.dtype.type, np.floating):
    image = to_uint8(image)
  with _open(path, &#39;wb&#39;) as f:
    _pil_image(image).save(f, format=&#39;png&#39;, **kwargs)


def to_rgb(
    array: Any,
    *,
    vmin: Optional[float] = None,
    vmax: Optional[float] = None,
    cmap: Union[str, Callable[[np.ndarray], np.ndarray]] = &#39;gray&#39;,
) -&gt; np.ndarray:
  &#34;&#34;&#34;Maps scalar values to RGB using value bounds and a color map.

  Args:
    array: Scalar values, with arbitrary shape.
    vmin: Explicit min value for remapping; if None, it is obtained as the
      minimum finite value of `array`.
    vmax: Explicit max value for remapping; if None, it is obtained as the
      maximum finite value of `array`.
    cmap: A pyplot color map or callable, to map from 1D value to 3D or 4D
      color.

  Returns:
    A new array in which each element is affinely mapped from [vmin, vmax]
    to [0.0, 1.0] and then color-mapped.
  &#34;&#34;&#34;
  array = _as_valid_media_array(array)
  # For future numpy version 1.7.0:
  # vmin = np.min(array, where=np.isfinite(array)) if vmin is None else vmin
  # vmax = np.max(array, where=np.isfinite(array)) if vmax is None else vmax
  vmin = np.min(np.where(np.isfinite(array), array,
                         np.inf)) if vmin is None else vmin
  vmax = np.max(np.where(np.isfinite(array), array,
                         -np.inf)) if vmax is None else vmax
  array = (array - vmin) / (vmax - vmin + np.finfo(float).eps)
  if isinstance(cmap, str):
    rgb_from_scalar = plt.cm.get_cmap(cmap)
  else:
    rgb_from_scalar = cmap
  array = rgb_from_scalar(array)
  # If there is a fully opaque alpha channel, remove it.
  if (array.shape[-1] == 4 and np.all(to_float01(array[..., 3])) == 1.0):
    array = array[..., :3]
  return array


def compress_image(image: np.ndarray,
                   *,
                   fmt: str = &#39;png&#39;,
                   **kwargs: Any) -&gt; bytes:
  &#34;&#34;&#34;Returns a buffer containing a compressed image.

  Args:
    image: Array in a format supported by PIL, e.g. np.uint8 or np.uint16.
    fmt: Desired compression encoding, e.g. &#39;png&#39;.
    **kwargs: Options for `PIL.save()`, e.g. `optimize=True` for greater
      compression.
  &#34;&#34;&#34;
  with io.BytesIO() as output:
    _pil_image(image).save(output, format=fmt, **kwargs)
    return output.getvalue()


def decompress_image(data: bytes, dtype: Any = None) -&gt; np.ndarray:
  &#34;&#34;&#34;Returns an image from a compressed data buffer.&#34;&#34;&#34;
  pil_image = PIL.ImageOps.exif_transpose(PIL.Image.open(io.BytesIO(data)))
  return np.array(pil_image, dtype=dtype)


def html_from_compressed_image(data: bytes,
                               width: int,
                               height: int,
                               *,
                               title: Optional[str] = None,
                               border: Union[bool, str] = False,
                               fmt: str = &#39;png&#39;) -&gt; str:
  &#34;&#34;&#34;Returns an HTML string with an image tag containing encoded data.

  Args:
    data: Compressed image bytes.
    width: Width of HTML image in pixels.
    height: Height of HTML image in pixels.
    title: Optional text shown centered above image.
    border: If `bool`, whether to place a black boundary around the image, or if
      `str`, the boundary CSS style.
    fmt: Compression encoding.
  &#34;&#34;&#34;
  b64 = base64.b64encode(data).decode(&#39;utf-8&#39;)
  border = (f&#39;{border}; &#39; if isinstance(border, str) else
            &#39;border:1px solid black; &#39; if border else &#39;&#39;)
  s = (f&#39;&lt;img width=&#34;{width}&#34; height=&#34;{height}&#34;&#39;
       f&#39; style=&#34;{border}image-rendering:pixelated; object-fit:cover;&#34;&#39;
       f&#39; src=&#34;data:image/{fmt};base64,{b64}&#34;/&gt;&#39;)
  if title:
    s = f&#34;&#34;&#34;&lt;div style=&#34;display:flex; align-items:left;&#34;&gt;
      &lt;div style=&#34;display:flex; flex-direction:column; align-items:center;&#34;&gt;
      &lt;div&gt;{title}&lt;/div&gt;&lt;div&gt;{s}&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&#34;&#34;&#34;
  return s


def _get_width_height(width: Optional[int], height: Optional[int],
                      shape: Tuple[int, int]) -&gt; Tuple[int, int]:
  &#34;&#34;&#34;Returns (width, height) given optional parameters and image shape.&#34;&#34;&#34;
  assert len(shape) == 2, shape
  if width and height:
    return width, height
  if width and not height:
    return width, int(width * (shape[0] / shape[1]) + 0.5)
  if height and not width:
    return int(height * (shape[1] / shape[0]) + 0.5), height
  return shape[::-1]


def show_image(image: Any,
               *,
               title: Optional[str] = None,
               **kwargs: Any) -&gt; None:
  &#34;&#34;&#34;Displays an image in the notebook and optionally saves it to a file.

  See `show_images`.

  &gt;&gt;&gt; show_image(np.random.rand(100, 100))
  &gt;&gt;&gt; show_image(np.random.randint(0, 256, size=(80, 80, 3), dtype=&#39;uint8&#39;))
  &gt;&gt;&gt; show_image(np.random.rand(10, 10) - 0.5, cmap=&#39;bwr&#39;, height=100)
  &gt;&gt;&gt; show_image(read_image(&#39;/tmp/image.png&#39;))
  &gt;&gt;&gt; url = &#39;https://github.com/hhoppe/data/raw/main/image.png&#39;
  &gt;&gt;&gt; show_image(read_image(url))

  Args:
    image: 2D array-like, or 3D array-like with 1, 3, or 4 channels.
    title: Optional text shown centered above the image.
    **kwargs: See `show_images`.
  &#34;&#34;&#34;
  show_images([image], [title], **kwargs)


def show_images(
    images: Union[Iterable[np.ndarray], Mapping[str, np.ndarray]],
    titles: Optional[Iterable[Optional[str]]] = None,
    *,
    width: Optional[int] = None,
    height: Optional[int] = None,
    downsample: bool = True,
    columns: Optional[int] = None,
    vmin: Optional[float] = None,
    vmax: Optional[float] = None,
    cmap: Union[str, Callable[[np.ndarray], np.ndarray]] = &#39;gray&#39;,
    border: Union[bool, str] = False,
) -&gt; None:
  &#34;&#34;&#34;Displays a row of images in the IPython/Jupyter notebook.

  If ```show_save.dir``` is not None, saves each titled image to a file based
  on the title.

  &gt;&gt;&gt; image1, image2 = np.random.rand(64, 64, 3), color_ramp((64, 64))
  &gt;&gt;&gt; show_images([image1, image2])
  &gt;&gt;&gt; show_images({&#39;random image&#39;: image1, &#39;color ramp&#39;: image2}, height=128)
  &gt;&gt;&gt; show_images([image1, image2] * 5, columns=4, border=True)

  Args:
    images: Iterable of images, or dictionary of `{title: image}`.  Each image
      must be either a 2D array or a 3D array with 1, 3, or 4 channels.
    titles: Optional strings shown above the corresponding images.
    width: Optional, overrides displayed width (in pixels).
    height: Optional, overrides displayed height (in pixels).
    downsample: If True, each image whose width or height is greater than the
      specified `height` or `width` is resampled to the display resolution. This
      improves antialiasing and reduces the size of the notebook.
    columns: Optional, maximum number of images per row.
    vmin: For single-channel image, explicit min value for display.
    vmax: For single-channel image, explicit max value for display.
    cmap: For single-channel image, pyplot color map or callable to map 1D to 3D
      color.
    border: If `bool`, whether to place a black boundary around the image, or if
      `str`, the boundary CSS style.
  &#34;&#34;&#34;
  if isinstance(images, collections.abc.Mapping):
    if titles is not None:
      raise ValueError(&#39;Cannot have images dictionary and titles parameter.&#39;)
    list_titles, list_images = list(images.keys()), list(images.values())
  else:
    list_images: List[np.ndarray] = list(images)  # type: ignore
    if titles is None:
      list_titles = [None] * len(list_images)
    else:
      list_titles = list(titles)
      if len(list_images) != len(list_titles):
        raise ValueError(&#39;Number of images does not match number of titles&#39;
                         f&#39; ({len(list_images)} vs {len(list_titles)}).&#39;)

  def ensure_mapped_to_rgb(image: Any) -&gt; np.ndarray:
    image = _as_valid_media_array(image)
    if not (image.ndim == 2 or
            (image.ndim == 3 and image.shape[2] in (1, 3, 4))):
      raise ValueError(f&#39;Image with shape {image.shape} is neither a 2D array&#39;
                       &#39; nor a 3D array with 1, 3, or 4 channels.&#39;)
    if image.ndim == 3 and image.shape[2] == 1:
      image = image[:, :, 0]
    if image.ndim == 2:
      image = to_rgb(image, vmin=vmin, vmax=vmax, cmap=cmap)
    return image

  list_images = [ensure_mapped_to_rgb(image) for image in list_images]

  def maybe_downsample(image: np.ndarray) -&gt; np.ndarray:
    w, h = _get_width_height(width, height, image.shape[:2])
    if w &lt; image.shape[1] or h &lt; image.shape[0]:
      image = resize_image(image, (h, w))
    return image

  if downsample:
    list_images = [maybe_downsample(image) for image in list_images]
  png_datas = [compress_image(to_uint8(image)) for image in list_images]

  for title, png_data in zip(list_titles, png_datas):
    if title and show_save.dir:
      path = pathlib.Path(show_save.dir) / f&#39;{title}.png&#39;
      with _open(path, mode=&#39;wb&#39;) as f:
        f.write(png_data)

  def html_from_compressed_images() -&gt; str:
    html_strings = []
    for image, title, png_data in zip(list_images, list_titles, png_datas):
      w, h = _get_width_height(width, height, image.shape[:2])
      html_strings.append(
          html_from_compressed_image(
              png_data, w, h, title=title, border=border))
    # Create single-row tables each with no more than &#39;columns&#39; elements.
    table_strings = []
    for row_html_strings in _chunked(html_strings, columns):
      s = &#39;&#39;.join(f&#39;&lt;td&gt;{e}&lt;/td&gt;&#39; for e in row_html_strings)
      table_strings.append(
          f&#39;&lt;table style=&#34;border-spacing:0;&#34;&gt;&lt;tr&gt;{s}&lt;/tr&gt;&lt;/table&gt;&#39;)
    return &#39;&#39;.join(table_strings)

  s = html_from_compressed_images()
  while len(s) &gt; _IPYTHON_HTML_SIZE_LIMIT * 0.5:
    list_images = [image[::2, ::2] for image in list_images]
    png_datas = [compress_image(to_uint8(image)) for image in list_images]
    s = html_from_compressed_images()
  IPython.display.display(IPython.display.HTML(s))


## Video I/O.


def _filename_suffix_from_codec(codec: str) -&gt; str:
  return &#39;.gif&#39; if codec == &#39;gif&#39; else &#39;.mp4&#39;


def _get_ffmpeg_path() -&gt; str:
  path = _search_for_ffmpeg_path()
  if not path:
    raise RuntimeError(
        f&#34;Program &#39;{FFMPEG_NAME}&#39; is not found; use &#39;apt-get install ffmpeg&#39;.&#34;)
  return path


def video_is_available() -&gt; bool:
  &#34;&#34;&#34;Returns True if the program `FFMPEG_NAME` is found.&#34;&#34;&#34;
  return _search_for_ffmpeg_path() is not None


class VideoMetadata(typing.NamedTuple):
  &#34;&#34;&#34;Represents the data stored in a video container header.

  Attributes:
    num_images: Number of frames that is expected from the video stream.  This
      is estimated from the framerate and the duration stored in the video
      header, so it might be inexact.
    shape: The dimensions (height, width) of each video frame.
    fps: The framerate in frames per second.
    bps: The estimated bitrate of the video stream in bits per second, retrieved
      from the video header.
  &#34;&#34;&#34;
  num_images: int
  shape: Tuple[int, int]
  fps: float
  bps: Optional[int]


def _get_video_metadata(path: _StrOrPath) -&gt; VideoMetadata:
  &#34;&#34;&#34;Returns attributes of video stored in the specified local file.&#34;&#34;&#34;
  if not pathlib.Path(path).is_file():
    raise RuntimeError(f&#34;Video file &#39;{path}&#39; is not found.&#34;)
  command = [
      _get_ffmpeg_path(), &#39;-nostdin&#39;, &#39;-i&#39;,
      str(path), &#39;-acodec&#39;, &#39;copy&#39;, &#39;-vcodec&#39;, &#39;copy&#39;, &#39;-f&#39;, &#39;null&#39;, &#39;-&#39;
  ]
  with subprocess.Popen(
      command, stderr=subprocess.PIPE, encoding=&#39;utf-8&#39;) as proc:
    _, err = proc.communicate()
  bps = num_images = width = rotation = None
  for line in err.split(&#39;\n&#39;):
    match = re.search(r&#39;, bitrate: *([0-9.]+) kb/s&#39;, line)
    if match:
      bps = int(match.group(1)) * 1000
    matches = re.findall(r&#39;frame= *([0-9]+) &#39;, line)
    if matches:
      num_images = int(matches[-1])
    if &#39;Stream #0:&#39; in line and &#39;: Video:&#39; in line:
      match = re.search(r&#39;, ([0-9]+)x([0-9]+)&#39;, line)
      if not match:
        raise RuntimeError(f&#39;Unable to parse video dimensions in line {line}&#39;)
      width, height = int(match.group(1)), int(match.group(2))
      match = re.search(r&#39;, ([0-9.]+) fps&#39;, line)
      if not match:
        raise RuntimeError(f&#39;Unable to parse video framerate in line {line}&#39;)
      fps = float(match.group(1))
    match = re.fullmatch(r&#39;\s*rotate\s*:\s*(\d+)&#39;, line)
    if match:
      rotation = int(match.group(1))
  if not num_images:
    raise RuntimeError(f&#39;Unable to find frames in video: {err}&#39;)
  if not width:
    raise RuntimeError(f&#39;Unable to parse video header: {err}&#39;)
  # By default, ffmpeg enables &#34;-autorotate&#34;; we just fix the dimensions.
  if rotation in (90, 270):
    width, height = height, width
  shape = (height, width)
  return VideoMetadata(num_images, shape, fps, bps)


class VideoIO:
  &#34;&#34;&#34;Base class for `VideoReader` and `VideoWriter`.&#34;&#34;&#34;

  def _get_pix_fmt(self, dtype: Any, image_format: str) -&gt; str:
    &#34;&#34;&#34;Returns ffmpeg pix_fmt given data type and image format.&#34;&#34;&#34;
    native_endian_suffix = {&#39;little&#39;: &#39;le&#39;, &#39;big&#39;: &#39;be&#39;}[sys.byteorder]
    return {
        np.uint8: {
            &#39;rgb&#39;: &#39;rgb24&#39;,
            &#39;yuv&#39;: &#39;yuv444p&#39;,
            &#39;gray&#39;: &#39;gray&#39;,
        },
        np.uint16: {
            &#39;rgb&#39;: &#39;rgb48&#39; + native_endian_suffix,
            &#39;yuv&#39;: &#39;yuv444p16&#39; + native_endian_suffix,
            &#39;gray&#39;: &#39;gray16&#39; + native_endian_suffix,
        },
    }[dtype.type][image_format]


class VideoReader(VideoIO, ContextManager[Any]):
  &#34;&#34;&#34;Context to read a compressed video as an iterable over its images.

  &gt;&gt;&gt; with VideoReader(&#39;/tmp/river.mp4&#39;) as reader:
  ...   print(f&#39;Video has {reader.num_images} images with shape={reader.shape},&#39;
  ...         f&#39; at {reader.fps} frames/sec and {reader.bps} bits/sec.&#39;)
  ...   for image in reader:
  ...     print(image.shape)

  Attributes:
    path_or_url: Location of input video.
    output_format: Format of output images (default &#39;rgb&#39;):
      - &#39;rgb&#39;: Each image has shape=(height, width, 3) with R, G, B values.
      - &#39;yuv&#39;: Each image has shape=(height, width, 3) with Y, U, V values.
      - &#39;gray&#39;: Each image has shape=(height, width).
    dtype: Data type for output images:
      - np.uint8: Default.
      - np.uint16: Allows reading 10-bit or 12-bit data without precision loss.
    metadata: Object storing the information retrieved from the video header.
      Its attributes are copied as attributes in this class.
    num_images: Number of frames that is expected from the video stream.  This
      is estimated from the framerate and the duration stored in the video
      header, so it might be inexact.
    shape: The dimensions (height, width) of each video frame.
    fps: The framerate in frames per second.
    bps: The estimated bitrate of the video stream in bits per second, retrieved
      from the video header.
  &#34;&#34;&#34;
  path_or_url: _StrOrPath
  output_format: str
  dtype: Any
  metadata: VideoMetadata
  num_images: int
  shape: Tuple[int, int]
  fps: float
  bps: Optional[int]
  _num_bytes_per_image: int

  def __init__(self,
               path_or_url: _StrOrPath,
               *,
               output_format: str = &#39;rgb&#39;,
               dtype: Any = np.uint8):
    &#34;&#34;&#34;Initializes video reading from the specified path or url.&#34;&#34;&#34;
    if output_format not in {&#39;rgb&#39;, &#39;yuv&#39;, &#39;gray&#39;}:
      raise ValueError(
          f&#39;Output format {output_format} is not rgb, yuv, or gray.&#39;)
    self.path_or_url = path_or_url
    self.output_format = output_format
    self.dtype = np.dtype(dtype)
    if self.dtype not in (np.uint8, np.uint16):
      raise ValueError(f&#39;Type {dtype} is not np.uint8 or np.uint16.&#39;)
    self._read_via_local_file: Any = None
    self._popen: Optional[&#39;subprocess.Popen[bytes]&#39;] = None
    self._proc: Optional[&#39;subprocess.Popen[bytes]&#39;] = None

  def __enter__(self) -&gt; &#39;VideoReader&#39;:
    ffmpeg_path = _get_ffmpeg_path()
    try:
      self._read_via_local_file = read_via_local_file(self.path_or_url)
      tmp_name = self._read_via_local_file.__enter__()

      self.metadata = _get_video_metadata(tmp_name)
      self.num_images, self.shape, self.fps, self.bps = self.metadata
      pix_fmt = self._get_pix_fmt(self.dtype, self.output_format)
      num_channels = {&#39;rgb&#39;: 3, &#39;yuv&#39;: 3, &#39;gray&#39;: 1}[self.output_format]
      bytes_per_channel = self.dtype.itemsize
      self._num_bytes_per_image = (
          np.prod(self.shape) * num_channels * bytes_per_channel)

      command = [
          ffmpeg_path, &#39;-v&#39;, &#39;panic&#39;, &#39;-nostdin&#39;, &#39;-i&#39;, tmp_name, &#39;-vcodec&#39;,
          &#39;rawvideo&#39;, &#39;-f&#39;, &#39;image2pipe&#39;, &#39;-pix_fmt&#39;, pix_fmt, &#39;-&#39;
      ]
      self._popen = subprocess.Popen(
          command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
      self._proc = self._popen.__enter__()
    except BaseException:
      self.__exit__(None, None, None)
      raise
    return self

  def __exit__(self, *_: Any) -&gt; None:
    self.close()

  def read(self) -&gt; Optional[np.ndarray]:
    &#34;&#34;&#34;Reads a video image frame (or None if at end of file).&#34;&#34;&#34;
    assert self._proc, &#39;Error: reading from an already closed context.&#39;
    assert self._proc.stdout
    data = self._proc.stdout.read(self._num_bytes_per_image)
    if not data:  # Due to either end-of-file or subprocess error.
      self.close()  # Raises exception if subprocess had error.
      return None  # To indicate end-of-file.
    assert len(data) == self._num_bytes_per_image
    image = np.frombuffer(data, dtype=self.dtype)
    if self.output_format == &#39;rgb&#39;:
      image = image.reshape(*self.shape, 3)
    elif self.output_format == &#39;yuv&#39;:  # Convert from planar YUV to pixel YUV.
      image = np.moveaxis(image.reshape(3, *self.shape), 0, 2)
    elif self.output_format == &#39;gray&#39;:  # Generate 2D rather than 3D ndimage.
      image = image.reshape(*self.shape)
    else:
      raise AssertionError
    return image

  def __iter__(self) -&gt; Generator[np.ndarray, None, None]:
    while True:
      image = self.read()
      if image is None:
        return
      yield image

  def close(self) -&gt; None:
    &#34;&#34;&#34;Terminates the piped subprocess reader.&#34;&#34;&#34;
    if self._popen:
      self._popen.__exit__(None, None, None)
      self._popen = None
      self._proc = None
    if self._read_via_local_file:
      self._read_via_local_file.__exit__(None, None, None)
      self._read_via_local_file = None


class VideoWriter(VideoIO, ContextManager[Any]):
  &#34;&#34;&#34;Context to write a compressed video.

  &gt;&gt;&gt; shape = (480, 640)
  &gt;&gt;&gt; with VideoWriter(&#39;/tmp/v.mp4&#39;, shape, fps=60) as writer:
  ...   for image in moving_circle(shape, num_images=60):
  ...     writer.add_image(image)
  &gt;&gt;&gt; show_video(read_video(&#39;/tmp/v.mp4&#39;))


  Bitrate control may be specified using at most one of: `bps`, `qp`, or `crf`.
  If none are specified, `qp` is set to a default value.
  See https://slhck.info/video/2017/03/01/rate-control.html

  If codec is &#39;gif&#39;, the args `bps`, `qp`, `crf`, and `encoded_format` are
  ignored.

  Attributes:
    path: Output video.  Its suffix (e.g. &#39;.mp4&#39;) determines the video container
      format.  The suffix must be &#39;.gif&#39; if the codec is &#39;gif&#39;.
    shape: 2D spatial dimensions (height, width) of video image frames.  The
      dimensions must be even if &#39;encoded_format&#39; has subsampled chroma (e.g.,
      &#39;yuv420p&#39; or &#39;yuv420p10le&#39;).
    fps: Frames-per-second frame rate (default is 60.0 except 25.0 for &#39;gif&#39;).
    codec: Compression algorithm as defined by &#34;ffmpeg -codecs&#34; (e.g., &#39;h264&#39;,
      &#39;hevc&#39;, &#39;vp9&#39;, or &#39;gif&#39;).
    bps: Requested average bits-per-second bitrate (default None).
    qp: Quantization parameter for video compression quality (default None).
    crf: Constant rate factor for video compression quality (default None).
    ffmpeg_args: Additional arguments for `ffmpeg` command, e.g. &#39;-g 30&#39; to
      introduce I-frames, or &#39;-bf 0&#39; to omit B-frames.
    input_format: Format of input images (default &#39;rgb&#39;):
      - &#39;rgb&#39;: Each image has shape=(height, width, 3) or (height, width).
      - &#39;yuv&#39;: Each image has shape=(height, width, 3) with Y, U, V values.
      - &#39;gray&#39;: Each image has shape=(height, width).
    dtype: Expected data type for input images (any float input images are
      converted to `dtype`):
      - np.uint8: Default.
      - np.uint16: Necessary when encoding &gt;8 bits/channel.
    encoded_format: Pixel format as defined by `ffmpeg -pix_fmts`, e.g.,
      &#39;yuv420p&#39; (2x2-subsampled chroma), &#39;yuv444p&#39; (full-res chroma),
      &#39;yuv420p10le&#39; (10-bit per channel), etc.  The default (None) selects
      &#39;yuv420p&#39; if all shape dimensions are even, else &#39;yuv444p&#39;.
  &#34;&#34;&#34;

  def __init__(self,
               path: _StrOrPath,
               shape: Tuple[int, int],
               *,
               fps: Optional[float] = None,
               codec: str = &#39;h264&#39;,
               bps: Optional[int] = None,
               qp: Optional[int] = None,
               crf: Optional[float] = None,
               ffmpeg_args: Union[str, Sequence[str]] = &#39;&#39;,
               input_format: str = &#39;rgb&#39;,
               dtype: Any = np.uint8,
               encoded_format: Optional[str] = None) -&gt; None:
    &#34;&#34;&#34;Initializes video writing to the specified path.&#34;&#34;&#34;
    _check_2d_shape(shape)
    if fps is None:
      fps = 25.0 if codec == &#39;gif&#39; else 60.0
    if fps &lt;= 0.0:
      raise ValueError(f&#39;Frame-per-second value {fps} is invalid.&#39;)
    bps = int(bps) if bps is not None else None
    if bps is not None and bps &lt;= 0:
      raise ValueError(f&#39;Bitrate value {bps} is invalid.&#39;)
    if qp is not None and (not isinstance(qp, int) or qp &lt;= 0):
      raise ValueError(
          f&#39;Quantization parameter {qp} is not a positive integer.&#39;)
    num_rate_specifications = sum(x is not None for x in (bps, qp, crf))
    if num_rate_specifications &gt; 1:
      raise ValueError(
          f&#39;Must specify at most one of bps, qp, or crf ({bps}, {qp}, {crf}).&#39;)
    ffmpeg_args = (
        shlex.split(ffmpeg_args)
        if isinstance(ffmpeg_args, str) else list(ffmpeg_args))
    if input_format not in {&#39;rgb&#39;, &#39;yuv&#39;, &#39;gray&#39;}:
      raise ValueError(f&#39;Input format {input_format} is not rgb, yuv, or gray.&#39;)
    dtype = np.dtype(dtype)
    if dtype not in (np.uint8, np.uint16):
      raise ValueError(f&#39;Type {dtype} is not np.uint8 or np.uint16.&#39;)
    self.path = pathlib.Path(path)
    self.shape = shape
    all_dimensions_are_even = all(dim % 2 == 0 for dim in shape)
    if encoded_format is None:
      encoded_format = &#39;yuv420p&#39; if all_dimensions_are_even else &#39;yuv444p&#39;
    if not all_dimensions_are_even and encoded_format.startswith(
        (&#39;yuv42&#39;, &#39;yuvj42&#39;)):
      raise ValueError(
          f&#39;With encoded_format {encoded_format}, video dimensions must be&#39;
          f&#39; even, but shape is {shape}.&#39;)
    self.fps = fps
    self.codec = codec
    self.bps = bps
    self.qp = qp
    self.crf = crf
    self.ffmpeg_args = ffmpeg_args
    self.input_format = input_format
    self.dtype = dtype
    self.encoded_format = encoded_format
    if num_rate_specifications == 0 and not ffmpeg_args:
      qp = 20 if np.prod(self.shape) &lt;= 640 * 480 else 28
    self._bitrate_args = (
        ([&#39;-vb&#39;, f&#39;{bps}&#39;] if bps is not None else []) +
        ([&#39;-qp&#39;, f&#39;{qp}&#39;] if qp is not None else []) +
        ([&#39;-vb&#39;, &#39;0&#39;, &#39;-crf&#39;, f&#39;{crf}&#39;] if crf is not None else []))
    if self.codec == &#39;gif&#39;:
      if self.path.suffix != &#39;.gif&#39;:
        raise ValueError(f&#34;File &#39;{self.path}&#39; does not have a .gif suffix.&#34;)
      self.encoded_format = &#39;rgb24&#39;
      self._bitrate_args = []
      video_filter = &#39;split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse&#39;
      self.ffmpeg_args = [&#39;-vf&#39;, video_filter, &#39;-f&#39;, &#39;gif&#39;] + self.ffmpeg_args
    self._write_via_local_file: Any = None
    self._popen: Optional[&#39;subprocess.Popen[bytes]&#39;] = None
    self._proc: Optional[&#39;subprocess.Popen[bytes]&#39;] = None

  def __enter__(self) -&gt; &#39;VideoWriter&#39;:
    ffmpeg_path = _get_ffmpeg_path()
    input_pix_fmt = self._get_pix_fmt(self.dtype, self.input_format)
    try:
      self._write_via_local_file = write_via_local_file(self.path)
      tmp_name = self._write_via_local_file.__enter__()

      # Writing to stdout using (&#39;-f&#39;, &#39;mp4&#39;, &#39;-&#39;) would require
      # (&#39;-movflags&#39;, &#39;frag_keyframe+empty_moov&#39;) and the result is nonportable.
      height, width = self.shape
      command = [
          ffmpeg_path, &#39;-v&#39;, &#39;error&#39;, &#39;-f&#39;, &#39;rawvideo&#39;, &#39;-vcodec&#39;, &#39;rawvideo&#39;,
          &#39;-pix_fmt&#39;, input_pix_fmt, &#39;-s&#39;, f&#39;{width}x{height}&#39;, &#39;-r&#39;,
          f&#39;{self.fps}&#39;, &#39;-i&#39;, &#39;-&#39;, &#39;-an&#39;, &#39;-vcodec&#39;, self.codec, &#39;-pix_fmt&#39;,
          self.encoded_format
      ] + self._bitrate_args + self.ffmpeg_args + [&#39;-y&#39;, tmp_name]
      self._popen = subprocess.Popen(
          command, stdin=subprocess.PIPE, stderr=subprocess.PIPE)
      self._proc = self._popen.__enter__()
    except BaseException:
      self.__exit__(None, None, None)
      raise
    return self

  def __exit__(self, *_: Any) -&gt; None:
    self.close()

  def add_image(self, image: np.ndarray) -&gt; None:
    &#34;&#34;&#34;Writes a video frame.

    Args:
      image: Array whose dtype and first two dimensions must match the `dtype`
        and `shape` specified in `VideoWriter` initialization.  If
        `input_format` is &#39;gray&#39;, the image must be 2D.  For the &#39;rgb&#39;
        input_format, the image may be either 2D (interpreted as grayscale) or
        3D with three (R, G, B) channels.  For the &#39;yuv&#39; input_format, the image
        must be 3D with three (Y, U, V) channels.

    Raises:
      RuntimeError: If there is an error writing to the output file.
    &#34;&#34;&#34;
    assert self._proc, &#39;Error: writing to an already closed context.&#39;
    if issubclass(image.dtype.type, (np.floating, np.bool_)):
      image = to_uint(image, self.dtype)
    if image.dtype != self.dtype:
      raise ValueError(f&#39;Image type {image.dtype} != {self.dtype}.&#39;)
    if self.input_format == &#39;gray&#39;:
      if image.ndim != 2:
        raise ValueError(f&#39;Image dimensions {image.shape} are not 2D.&#39;)
    else:
      if image.ndim == 2 and self.input_format == &#39;rgb&#39;:
        image = np.dstack((image, image, image))
      if not (image.ndim == 3 and image.shape[2] == 3):
        raise ValueError(&#39;Image dimensions {image.shape} are invalid.&#39;)
    if image.shape[:2] != self.shape:
      raise ValueError(f&#39;Image dimensions {image.shape[:2]} do not match&#39;
                       f&#39; those of the initialized video {self.shape}.&#39;)
    if self.input_format == &#39;yuv&#39;:  # Convert from per-pixel YUV to planar YUV.
      image = np.moveaxis(image, 2, 0)
    data = image.tobytes()
    assert self._proc.stdin
    if self._proc.stdin.write(data) != len(data):
      self._proc.wait()
      assert self._proc.stderr
      s = self._proc.stderr.read().decode()
      raise RuntimeError(f&#34;Error writing &#39;{self.path}&#39;: {s}&#34;)

  def close(self) -&gt; None:
    &#34;&#34;&#34;Finishes writing the video.&#34;&#34;&#34;
    if self._popen:
      assert self._proc and self._proc.stdin and self._proc.stderr
      self._proc.stdin.close()
      if self._proc.wait():
        s = self._proc.stderr.read().decode()
        raise RuntimeError(f&#34;Error writing &#39;{self.path}&#39;: {s}&#34;)
      self._popen.__exit__(None, None, None)
      self._popen = None
      self._proc = None
    if self._write_via_local_file:
      self._write_via_local_file.__exit__(None, None, None)
      self._write_via_local_file = None


def read_video(path_or_url: _StrOrPath, **kwargs: Any) -&gt; np.ndarray:
  &#34;&#34;&#34;Returns a 4D array containing all images from a compressed video file.

  &gt;&gt;&gt; video = read_video(&#39;/tmp/river.mp4&#39;)
  &gt;&gt;&gt; show_video(video)
  &gt;&gt;&gt; url = &#39;https://github.com/hhoppe/data/raw/main/video.mp4&#39;
  &gt;&gt;&gt; show_video(read_video(url))

  Args:
    path_or_url: Input video file.
    **kwargs: Additional parameters for `VideoReader`.

  Returns:
    A 4D array with dimensions (frame, height, width, channel).
  &#34;&#34;&#34;
  with VideoReader(path_or_url, **kwargs) as reader:
    return np.array(tuple(reader))


def write_video(path: _StrOrPath, images: Iterable[np.ndarray],
                **kwargs: Any) -&gt; None:
  &#34;&#34;&#34;Writes images to a compressed video file.

  &gt;&gt;&gt; video = moving_circle((480, 640), num_images=60)
  &gt;&gt;&gt; write_video(&#39;/tmp/v.mp4&#39;, video, fps=60.0, qp=18)
  &gt;&gt;&gt; show_video(read_video(&#39;/tmp/v.mp4&#39;))

  Args:
    path: Output video file.
    images: Iterable over video frames, e.g. a 4D array or a list of 2D or 3D
      arrays.
    **kwargs: Additional parameters for `VideoWriter`.
  &#34;&#34;&#34;
  first_image, images = peek_first(images)
  shape = first_image.shape[:2]
  dtype = first_image.dtype
  if dtype == np.bool:
    dtype = np.uint8
  elif issubclass(dtype.type, np.floating):
    dtype = np.uint16
  with VideoWriter(path, shape=shape, dtype=dtype, **kwargs) as writer:
    for image in images:
      writer.add_image(image)


def compress_video(images: Iterable[np.ndarray],
                   *,
                   codec: str = &#39;h264&#39;,
                   **kwargs: Any) -&gt; bytes:
  &#34;&#34;&#34;Returns a buffer containing a compressed video.

  The video container is &#39;mp4&#39; except when `codec` is &#39;gif&#39;.

  &gt;&gt;&gt; video = read_video(&#39;/tmp/river.mp4&#39;)
  &gt;&gt;&gt; data = compress_video(video, fps=30.0, bps=10_000_000)
  &gt;&gt;&gt; print(len(data))

  Args:
    images: Iterable over video frames.
    codec: Compression algorithm as defined by `ffmpeg -codecs` (e.g., &#39;h264&#39;,
      &#39;hevc&#39;, &#39;vp9&#39;, or &#39;gif&#39;).
    **kwargs: Additional parameters for `VideoWriter`.

  Returns:
    A bytes buffer containing the compressed video.
  &#34;&#34;&#34;
  suffix = _filename_suffix_from_codec(codec)
  with tempfile.TemporaryDirectory() as directory_name:
    tmp_path = pathlib.Path(directory_name) / f&#39;file{suffix}&#39;
    write_video(tmp_path, images, codec=codec, **kwargs)
    return tmp_path.read_bytes()


def decompress_video(data: bytes, **kwargs: Any) -&gt; np.ndarray:
  &#34;&#34;&#34;Returns video images from an MP4-compressed data buffer.&#34;&#34;&#34;
  with tempfile.TemporaryDirectory() as directory_name:
    tmp_path = pathlib.Path(directory_name) / &#39;file.mp4&#39;
    tmp_path.write_bytes(data)
    return read_video(tmp_path, **kwargs)


def html_from_compressed_video(data: bytes,
                               width: int,
                               height: int,
                               *,
                               title: Optional[str] = None,
                               border: Union[bool, str] = False,
                               loop: bool = True,
                               autoplay: bool = True) -&gt; str:
  &#34;&#34;&#34;Returns an HTML string with a video tag containing H264-encoded data.

  Args:
    data: MP4-compressed video bytes.
    width: Width of HTML video in pixels.
    height: Height of HTML video in pixels.
    title: Optional text shown centered above the video.
    border: If `bool`, whether to place a black boundary around the image, or if
      `str`, the boundary CSS style.
    loop: If True, the playback repeats forever.
    autoplay: If True, video playback starts without having to click.
  &#34;&#34;&#34;
  b64 = base64.b64encode(data).decode(&#39;utf-8&#39;)
  border = (f&#39;{border}; &#39; if isinstance(border, str) else
            &#39;border:1px solid black; &#39; if border else &#39;&#39;)
  options = (f&#39;controls width=&#34;{width}&#34; height=&#34;{height}&#34;&#39;
             f&#39; style=&#34;{border}object-fit:cover;&#34;&#39;
             f&#34;{&#39; loop&#39; if loop else &#39;&#39;}{&#39; autoplay&#39; if autoplay else &#39;&#39;}&#34;)
  s = f&#34;&#34;&#34;&lt;video {options}&gt;
      &lt;source src=&#34;data:video/x-m4v;base64,{b64}&#34; type=&#34;video/mp4&#34;/&gt;
      This browser does not support the video tag.
      &lt;/video&gt;&#34;&#34;&#34;
  if title:
    s = f&#34;&#34;&#34;&lt;div style=&#34;display:flex; align-items:left;&#34;&gt;
      &lt;div style=&#34;display:flex; flex-direction:column; align-items:center;&#34;&gt;
      &lt;div&gt;{title}&lt;/div&gt;&lt;div&gt;{s}&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&#34;&#34;&#34;
  return s


def show_video(images: Iterable[np.ndarray],
               *,
               title: Optional[str] = None,
               **kwargs: Any) -&gt; None:
  &#34;&#34;&#34;Displays a video in the IPython notebook and optionally saves it to a file.

  See `show_videos`.

  &gt;&gt;&gt; video = read_video(&#39;https://github.com/hhoppe/data/raw/main/video.mp4&#39;)
  &gt;&gt;&gt; show_video(video, title=&#39;River video&#39;, fps=10.0)

  &gt;&gt;&gt; show_video(moving_circle((80, 80), num_images=10), border=True)
  &gt;&gt;&gt; show_video(read_video(&#39;/tmp/river.mp4&#39;))

  Args:
    images: Iterable of video frames.
    title: Optional text shown centered above the video.
    **kwargs: See `show_videos`.
  &#34;&#34;&#34;
  show_videos([images], [title], **kwargs)


def show_videos(videos: Union[Iterable[Iterable[np.ndarray]],
                              Mapping[str, Iterable[np.ndarray]]],
                titles: Optional[Sequence[Optional[str]]] = None,
                *,
                width: Optional[int] = None,
                height: Optional[int] = None,
                downsample: bool = True,
                columns: Optional[int] = None,
                fps: Optional[float] = None,
                bps: Optional[int] = None,
                qp: Optional[int] = None,
                codec: str = &#39;h264&#39;,
                **kwargs: Any) -&gt; None:
  &#34;&#34;&#34;Displays a row of videos in the IPython notebook.

  Creates HTML with `&lt;video&gt;` tags containing embedded H264-encoded bytestrings.
  If `codec` is set to &#39;gif&#39;, we instead use `&lt;img&gt;` tags containing embedded
  GIF-encoded bytestrings.  Note that the resulting GIF animations skip frames
  when the `fps` period is not a multiple of 10 ms units (GIF frame delay
  units).  Encoding at `fps` = 20.0, 25.0, or 50.0 works fine.

  If ```show_save.dir``` is not None, saves each titled video to a file based
  on the title.

  Args:
    videos: Iterable of videos, or dictionary of `{title: video}`.  Each video
      must be an iterable of images.
    titles: Optional sequence of strings shown above the corresponding videos.
    width: Optional, overrides displayed width (in pixels).
    height: Optional, overrides displayed height (in pixels).
    downsample: If True, each video whose width or height is greater than the
      specified `height` or `width` is resampled to the display resolution. This
      improves antialiasing and reduces the size of the notebook.
    columns: Optional, maximum number of videos per row.
    fps: Frames-per-second frame rate (default is 60.0 except 25.0 for GIF).
    bps: Bits-per-second bitrate (default None).
    qp: Quantization parameter for video compression quality (default None).
    codec: Compression algorithm; must be either &#39;h264&#39; or &#39;gif&#39;.
    **kwargs: Additional parameters (`border`, `loop`, `autoplay`) for
      `html_from_compressed_video`.
  &#34;&#34;&#34;
  if isinstance(videos, collections.abc.Mapping):
    if titles is not None:
      raise ValueError(
          &#39;Cannot have both a video dictionary and a titles parameter.&#39;)
    list_titles, list_videos = list(videos.keys()), list(videos.values())
  else:
    list_videos: List[Iterable[np.ndarray]] = list(videos)  # type: ignore
    list_titles = [None] * len(list_videos) if titles is None else list(titles)
    if len(list_videos) != len(list_titles):
      raise ValueError(&#39;Number of videos does not match number of titles&#39;
                       f&#39; ({len(list_videos)} vs {len(list_titles)}).&#39;)
  if codec not in (&#39;h264&#39;, &#39;gif&#39;):
    raise ValueError(f&#39;Codec {codec} is neither h264 or gif.&#39;)

  html_strings = []
  for video, title in zip(list_videos, list_titles):
    first_image, video = peek_first(video)
    w, h = _get_width_height(width, height, first_image.shape[:2])
    if downsample and (w &lt; first_image.shape[1] or h &lt; first_image.shape[0]):
      # Not resize_video() because each image may have different depth and type.
      video = [resize_image(image, (h, w)) for image in video]
    data = compress_video(video, fps=fps, bps=bps, qp=qp, codec=codec)
    if title and show_save.dir:
      suffix = _filename_suffix_from_codec(codec)
      path = pathlib.Path(show_save.dir) / f&#39;{title}{suffix}&#39;
      with _open(path, mode=&#39;wb&#39;) as f:
        f.write(data)
    if codec == &#39;gif&#39;:
      html_string = html_from_compressed_image(
          data, w, h, title=title, fmt=&#39;gif&#39;, **kwargs)
    else:
      html_string = html_from_compressed_video(
          data, w, h, title=title, **kwargs)
    html_strings.append(html_string)

  # Create single-row tables each with no more than &#39;columns&#39; elements.
  table_strings = []
  for row_html_strings in _chunked(html_strings, columns):
    s = &#39;&#39;.join(f&#39;&lt;td&gt;{e}&lt;/td&gt;&#39; for e in row_html_strings)
    table_strings.append(
        f&#39;&lt;table style=&#34;border-spacing:0;&#34;&gt;&lt;tr&gt;{s}&lt;/tr&gt;&lt;/table&gt;&#39;)
  s = &#39;&#39;.join(table_strings)
  IPython.display.display(IPython.display.HTML(s))</code></pre>
</details>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-variables">Global variables</h2>
<dl>
<dt id="mediapy.show_save"><code class="name">var <span class="ident">show_save</span></code></dt>
<dd>
<div class="desc"><p>Functionality to save all titled output from <code>show_*()</code> calls into files.</p>
<p>If <code>dir</code> attribute of <code><a title="mediapy.show_save" href="#mediapy.show_save">show_save</a></code> is not None, all titled images and videos
displayed by <code><a title="mediapy.show_image" href="#mediapy.show_image">show_image()</a></code>, <code><a title="mediapy.show_images" href="#mediapy.show_images">show_images()</a></code>, <code><a title="mediapy.show_video" href="#mediapy.show_video">show_video()</a></code>, and <code><a title="mediapy.show_videos" href="#mediapy.show_videos">show_videos()</a></code> are
also saved as files within the specified directory.</p>
<p>The context <code>to_dir(directory_name)</code> temporarily assigns the
<code>dir</code> attribute:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; with show_save.to_dir('/tmp'):
...   show_image(image, title='image1')  # Creates /tmp/image1.png.
...   show_video(video, title='video2')  # Creates /tmp/video2.mp4.
</code></pre>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>dir</code></strong></dt>
<dd>Directory into which to save titled images and videos, or None.</dd>
</dl></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="mediapy.color_ramp"><code class="name flex">
<span>def <span class="ident">color_ramp</span></span>(<span>shape:Â Tuple[int,Â int]Â =Â (64, 64), *, dtype:Â AnyÂ =Â numpy.float32) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns an image of a red-green color gradient.</p>
<p>This is useful for quick experimentation and testing.
See also
<code><a title="mediapy.moving_circle" href="#mediapy.moving_circle">moving_circle()</a></code> to generate a sample video.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>shape</code></strong></dt>
<dd>2D spatial dimensions (height, width) of generated image.</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>Type (uint or floating) of resulting pixel values.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def color_ramp(shape: Tuple[int, int] = (64, 64), *,
               dtype: Any = np.float32) -&gt; np.ndarray:
  &#34;&#34;&#34;Returns an image of a red-green color gradient.

  This is useful for quick experimentation and testing.  See also
  `moving_circle` to generate a sample video.

  Args:
    shape: 2D spatial dimensions (height, width) of generated image.
    dtype: Type (uint or floating) of resulting pixel values.
  &#34;&#34;&#34;
  _check_2d_shape(shape)
  dtype = _as_valid_media_type(dtype)
  yx = (np.moveaxis(np.indices(shape), 0, -1) + 0.5) / shape
  image = np.insert(yx, 2, 0.0, axis=-1)
  return to_type(image, dtype)</code></pre>
</details>
</dd>
<dt id="mediapy.compress_image"><code class="name flex">
<span>def <span class="ident">compress_image</span></span>(<span>image:Â numpy.ndarray, *, fmt:Â strÂ =Â 'png', **kwargs:Â Any) â€‘>Â bytes</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a buffer containing a compressed image.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>image</code></strong></dt>
<dd>Array in a format supported by PIL, e.g. np.uint8 or np.uint16.</dd>
<dt><strong><code>fmt</code></strong></dt>
<dd>Desired compression encoding, e.g. 'png'.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Options for <code>PIL.save()</code>, e.g. <code>optimize=True</code> for greater
compression.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compress_image(image: np.ndarray,
                   *,
                   fmt: str = &#39;png&#39;,
                   **kwargs: Any) -&gt; bytes:
  &#34;&#34;&#34;Returns a buffer containing a compressed image.

  Args:
    image: Array in a format supported by PIL, e.g. np.uint8 or np.uint16.
    fmt: Desired compression encoding, e.g. &#39;png&#39;.
    **kwargs: Options for `PIL.save()`, e.g. `optimize=True` for greater
      compression.
  &#34;&#34;&#34;
  with io.BytesIO() as output:
    _pil_image(image).save(output, format=fmt, **kwargs)
    return output.getvalue()</code></pre>
</details>
</dd>
<dt id="mediapy.compress_video"><code class="name flex">
<span>def <span class="ident">compress_video</span></span>(<span>images:Â Iterable[numpy.ndarray], *, codec:Â strÂ =Â 'h264', **kwargs:Â Any) â€‘>Â bytes</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a buffer containing a compressed video.</p>
<p>The video container is 'mp4' except when <code>codec</code> is 'gif'.</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; video = read_video('/tmp/river.mp4')
&gt;&gt;&gt; data = compress_video(video, fps=30.0, bps=10_000_000)
&gt;&gt;&gt; print(len(data))
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>images</code></strong></dt>
<dd>Iterable over video frames.</dd>
<dt><strong><code>codec</code></strong></dt>
<dd>Compression algorithm as defined by <code>ffmpeg -codecs</code> (e.g., 'h264',
'hevc', 'vp9', or 'gif').</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional parameters for <code><a title="mediapy.VideoWriter" href="#mediapy.VideoWriter">VideoWriter</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A bytes buffer containing the compressed video.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compress_video(images: Iterable[np.ndarray],
                   *,
                   codec: str = &#39;h264&#39;,
                   **kwargs: Any) -&gt; bytes:
  &#34;&#34;&#34;Returns a buffer containing a compressed video.

  The video container is &#39;mp4&#39; except when `codec` is &#39;gif&#39;.

  &gt;&gt;&gt; video = read_video(&#39;/tmp/river.mp4&#39;)
  &gt;&gt;&gt; data = compress_video(video, fps=30.0, bps=10_000_000)
  &gt;&gt;&gt; print(len(data))

  Args:
    images: Iterable over video frames.
    codec: Compression algorithm as defined by `ffmpeg -codecs` (e.g., &#39;h264&#39;,
      &#39;hevc&#39;, &#39;vp9&#39;, or &#39;gif&#39;).
    **kwargs: Additional parameters for `VideoWriter`.

  Returns:
    A bytes buffer containing the compressed video.
  &#34;&#34;&#34;
  suffix = _filename_suffix_from_codec(codec)
  with tempfile.TemporaryDirectory() as directory_name:
    tmp_path = pathlib.Path(directory_name) / f&#39;file{suffix}&#39;
    write_video(tmp_path, images, codec=codec, **kwargs)
    return tmp_path.read_bytes()</code></pre>
</details>
</dd>
<dt id="mediapy.decompress_image"><code class="name flex">
<span>def <span class="ident">decompress_image</span></span>(<span>data:Â bytes, dtype:Â AnyÂ =Â None) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns an image from a compressed data buffer.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def decompress_image(data: bytes, dtype: Any = None) -&gt; np.ndarray:
  &#34;&#34;&#34;Returns an image from a compressed data buffer.&#34;&#34;&#34;
  pil_image = PIL.ImageOps.exif_transpose(PIL.Image.open(io.BytesIO(data)))
  return np.array(pil_image, dtype=dtype)</code></pre>
</details>
</dd>
<dt id="mediapy.decompress_video"><code class="name flex">
<span>def <span class="ident">decompress_video</span></span>(<span>data:Â bytes, **kwargs:Â Any) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns video images from an MP4-compressed data buffer.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def decompress_video(data: bytes, **kwargs: Any) -&gt; np.ndarray:
  &#34;&#34;&#34;Returns video images from an MP4-compressed data buffer.&#34;&#34;&#34;
  with tempfile.TemporaryDirectory() as directory_name:
    tmp_path = pathlib.Path(directory_name) / &#39;file.mp4&#39;
    tmp_path.write_bytes(data)
    return read_video(tmp_path, **kwargs)</code></pre>
</details>
</dd>
<dt id="mediapy.html_from_compressed_image"><code class="name flex">
<span>def <span class="ident">html_from_compressed_image</span></span>(<span>data:Â bytes, width:Â int, height:Â int, *, title:Â Union[str,Â NoneType]Â =Â None, border:Â Union[bool,Â str]Â =Â False, fmt:Â strÂ =Â 'png') â€‘>Â str</span>
</code></dt>
<dd>
<div class="desc"><p>Returns an HTML string with an image tag containing encoded data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>Compressed image bytes.</dd>
<dt><strong><code>width</code></strong></dt>
<dd>Width of HTML image in pixels.</dd>
<dt><strong><code>height</code></strong></dt>
<dd>Height of HTML image in pixels.</dd>
<dt><strong><code>title</code></strong></dt>
<dd>Optional text shown centered above image.</dd>
<dt><strong><code>border</code></strong></dt>
<dd>If <code>bool</code>, whether to place a black boundary around the image, or if
<code>str</code>, the boundary CSS style.</dd>
<dt><strong><code>fmt</code></strong></dt>
<dd>Compression encoding.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def html_from_compressed_image(data: bytes,
                               width: int,
                               height: int,
                               *,
                               title: Optional[str] = None,
                               border: Union[bool, str] = False,
                               fmt: str = &#39;png&#39;) -&gt; str:
  &#34;&#34;&#34;Returns an HTML string with an image tag containing encoded data.

  Args:
    data: Compressed image bytes.
    width: Width of HTML image in pixels.
    height: Height of HTML image in pixels.
    title: Optional text shown centered above image.
    border: If `bool`, whether to place a black boundary around the image, or if
      `str`, the boundary CSS style.
    fmt: Compression encoding.
  &#34;&#34;&#34;
  b64 = base64.b64encode(data).decode(&#39;utf-8&#39;)
  border = (f&#39;{border}; &#39; if isinstance(border, str) else
            &#39;border:1px solid black; &#39; if border else &#39;&#39;)
  s = (f&#39;&lt;img width=&#34;{width}&#34; height=&#34;{height}&#34;&#39;
       f&#39; style=&#34;{border}image-rendering:pixelated; object-fit:cover;&#34;&#39;
       f&#39; src=&#34;data:image/{fmt};base64,{b64}&#34;/&gt;&#39;)
  if title:
    s = f&#34;&#34;&#34;&lt;div style=&#34;display:flex; align-items:left;&#34;&gt;
      &lt;div style=&#34;display:flex; flex-direction:column; align-items:center;&#34;&gt;
      &lt;div&gt;{title}&lt;/div&gt;&lt;div&gt;{s}&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&#34;&#34;&#34;
  return s</code></pre>
</details>
</dd>
<dt id="mediapy.html_from_compressed_video"><code class="name flex">
<span>def <span class="ident">html_from_compressed_video</span></span>(<span>data:Â bytes, width:Â int, height:Â int, *, title:Â Union[str,Â NoneType]Â =Â None, border:Â Union[bool,Â str]Â =Â False, loop:Â boolÂ =Â True, autoplay:Â boolÂ =Â True) â€‘>Â str</span>
</code></dt>
<dd>
<div class="desc"><p>Returns an HTML string with a video tag containing H264-encoded data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>MP4-compressed video bytes.</dd>
<dt><strong><code>width</code></strong></dt>
<dd>Width of HTML video in pixels.</dd>
<dt><strong><code>height</code></strong></dt>
<dd>Height of HTML video in pixels.</dd>
<dt><strong><code>title</code></strong></dt>
<dd>Optional text shown centered above the video.</dd>
<dt><strong><code>border</code></strong></dt>
<dd>If <code>bool</code>, whether to place a black boundary around the image, or if
<code>str</code>, the boundary CSS style.</dd>
<dt><strong><code>loop</code></strong></dt>
<dd>If True, the playback repeats forever.</dd>
<dt><strong><code>autoplay</code></strong></dt>
<dd>If True, video playback starts without having to click.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def html_from_compressed_video(data: bytes,
                               width: int,
                               height: int,
                               *,
                               title: Optional[str] = None,
                               border: Union[bool, str] = False,
                               loop: bool = True,
                               autoplay: bool = True) -&gt; str:
  &#34;&#34;&#34;Returns an HTML string with a video tag containing H264-encoded data.

  Args:
    data: MP4-compressed video bytes.
    width: Width of HTML video in pixels.
    height: Height of HTML video in pixels.
    title: Optional text shown centered above the video.
    border: If `bool`, whether to place a black boundary around the image, or if
      `str`, the boundary CSS style.
    loop: If True, the playback repeats forever.
    autoplay: If True, video playback starts without having to click.
  &#34;&#34;&#34;
  b64 = base64.b64encode(data).decode(&#39;utf-8&#39;)
  border = (f&#39;{border}; &#39; if isinstance(border, str) else
            &#39;border:1px solid black; &#39; if border else &#39;&#39;)
  options = (f&#39;controls width=&#34;{width}&#34; height=&#34;{height}&#34;&#39;
             f&#39; style=&#34;{border}object-fit:cover;&#34;&#39;
             f&#34;{&#39; loop&#39; if loop else &#39;&#39;}{&#39; autoplay&#39; if autoplay else &#39;&#39;}&#34;)
  s = f&#34;&#34;&#34;&lt;video {options}&gt;
      &lt;source src=&#34;data:video/x-m4v;base64,{b64}&#34; type=&#34;video/mp4&#34;/&gt;
      This browser does not support the video tag.
      &lt;/video&gt;&#34;&#34;&#34;
  if title:
    s = f&#34;&#34;&#34;&lt;div style=&#34;display:flex; align-items:left;&#34;&gt;
      &lt;div style=&#34;display:flex; flex-direction:column; align-items:center;&#34;&gt;
      &lt;div&gt;{title}&lt;/div&gt;&lt;div&gt;{s}&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&#34;&#34;&#34;
  return s</code></pre>
</details>
</dd>
<dt id="mediapy.moving_circle"><code class="name flex">
<span>def <span class="ident">moving_circle</span></span>(<span>shape:Â Tuple[int,Â int]Â =Â (256, 256), num_images:Â intÂ =Â 10, *, dtype:Â AnyÂ =Â numpy.float32) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a video of a circle moving in front of a color ramp.</p>
<p>This is useful for quick experimentation and testing.
See also <code><a title="mediapy.color_ramp" href="#mediapy.color_ramp">color_ramp()</a></code>
to generate a sample image.</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; show_video(moving_circle((480, 640), 60))
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>shape</code></strong></dt>
<dd>2D spatial dimensions (height, width) of generated video.</dd>
<dt><strong><code>num_images</code></strong></dt>
<dd>Number of video frames.</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>Type (uint or floating) of resulting pixel values.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def moving_circle(shape: Tuple[int, int] = (256, 256),
                  num_images: int = 10,
                  *,
                  dtype: Any = np.float32) -&gt; np.ndarray:
  &#34;&#34;&#34;Returns a video of a circle moving in front of a color ramp.

  This is useful for quick experimentation and testing.  See also `color_ramp`
  to generate a sample image.

  &gt;&gt;&gt; show_video(moving_circle((480, 640), 60))

  Args:
    shape: 2D spatial dimensions (height, width) of generated video.
    num_images: Number of video frames.
    dtype: Type (uint or floating) of resulting pixel values.
  &#34;&#34;&#34;
  _check_2d_shape(shape)
  dtype = np.dtype(dtype)

  def generate_image(image_index: int) -&gt; np.ndarray:
    &#34;&#34;&#34;Returns a video frame image.&#34;&#34;&#34;
    image = color_ramp(shape, dtype=dtype)
    yx = np.moveaxis(np.indices(shape), 0, -1)
    center = (shape[0] * 0.6, shape[1] * (image_index + 0.5) / num_images)
    radius_squared = (min(shape) * 0.1)**2
    inside = np.sum((yx - center)**2, axis=-1) &lt; radius_squared
    white_circle_color = (1.0, 1.0, 1.0)
    if issubclass(dtype.type, np.unsignedinteger):
      white_circle_color = to_uint([white_circle_color], dtype)[0]
    image[inside] = white_circle_color
    return image

  return np.array([generate_image(i) for i in range(num_images)])</code></pre>
</details>
</dd>
<dt id="mediapy.peek_first"><code class="name flex">
<span>def <span class="ident">peek_first</span></span>(<span>iterator:Â Iterable[~_T]) â€‘>Â Tuple[~_T,Â Iterable[~_T]]</span>
</code></dt>
<dd>
<div class="desc"><p>Given an iterator, returns first element and re-initialized iterator.</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; first_image, images = peek_first(moving_circle())
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>iterator</code></strong></dt>
<dd>An input iterator or iterable.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt>A tuple (first_element, iterator_reinitialized) containing:</dt>
<dt><code>
first_element</code></dt>
<dd>The first element of the input.
iterator_reinitialized: A clone of the original iterator/iterable.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def peek_first(iterator: Iterable[_T]) -&gt; Tuple[_T, Iterable[_T]]:
  &#34;&#34;&#34;Given an iterator, returns first element and re-initialized iterator.

  &gt;&gt;&gt; first_image, images = peek_first(moving_circle())

  Args:
    iterator: An input iterator or iterable.

  Returns:
    A tuple (first_element, iterator_reinitialized) containing:
      first_element: The first element of the input.
      iterator_reinitialized: A clone of the original iterator/iterable.
  &#34;&#34;&#34;
  # Inspired from https://stackoverflow.com/a/12059829/1190077
  peeker, iterator_reinitialized = itertools.tee(iterator)
  first = next(peeker)
  return first, iterator_reinitialized</code></pre>
</details>
</dd>
<dt id="mediapy.read_contents"><code class="name flex">
<span>def <span class="ident">read_contents</span></span>(<span>path_or_url:Â Union[str,Â ForwardRef('os.PathLike[str]')]) â€‘>Â bytes</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the contents of the file specified by either a path or URL.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_contents(path_or_url: _StrOrPath) -&gt; bytes:
  &#34;&#34;&#34;Returns the contents of the file specified by either a path or URL.&#34;&#34;&#34;
  data: bytes
  if _is_url(path_or_url):
    assert isinstance(path_or_url, str)
    with urllib.request.urlopen(path_or_url) as response:
      data = response.read()
  else:
    with _open(path_or_url, &#39;rb&#39;) as f:
      data = f.read()
  return data</code></pre>
</details>
</dd>
<dt id="mediapy.read_image"><code class="name flex">
<span>def <span class="ident">read_image</span></span>(<span>path_or_url:Â Union[str,Â ForwardRef('os.PathLike[str]')], *, dtype:Â AnyÂ =Â numpy.uint8) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns an image read from a file path or URL.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_image(path_or_url: _StrOrPath, *, dtype: Any = np.uint8) -&gt; np.ndarray:
  &#34;&#34;&#34;Returns an image read from a file path or URL.&#34;&#34;&#34;
  dtype = np.dtype(dtype)
  data = read_contents(path_or_url)
  return decompress_image(data, dtype)</code></pre>
</details>
</dd>
<dt id="mediapy.read_via_local_file"><code class="name flex">
<span>def <span class="ident">read_via_local_file</span></span>(<span>path_or_url:Â Union[str,Â ForwardRef('os.PathLike[str]')]) â€‘>Â Generator[str,Â NoneType,Â NoneType]</span>
</code></dt>
<dd>
<div class="desc"><p>Context to copy a remote file locally to read from it.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path_or_url</code></strong></dt>
<dd>File, which may be remote.</dd>
</dl>
<h2 id="yields">Yields</h2>
<p>The name of a local file which may be a copy of a remote file.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@contextlib.contextmanager
def read_via_local_file(path_or_url: _StrOrPath) -&gt; Generator[str, None, None]:
  &#34;&#34;&#34;Context to copy a remote file locally to read from it.

  Args:
    path_or_url: File, which may be remote.

  Yields:
    The name of a local file which may be a copy of a remote file.
  &#34;&#34;&#34;
  if _is_url(path_or_url) or not _path_is_local(path_or_url):
    suffix = pathlib.Path(path_or_url).suffix
    with tempfile.TemporaryDirectory() as directory_name:
      tmp_path = pathlib.Path(directory_name) / f&#39;file{suffix}&#39;
      tmp_path.write_bytes(read_contents(path_or_url))
      yield str(tmp_path)
  else:
    yield str(path_or_url)</code></pre>
</details>
</dd>
<dt id="mediapy.read_video"><code class="name flex">
<span>def <span class="ident">read_video</span></span>(<span>path_or_url:Â Union[str,Â ForwardRef('os.PathLike[str]')], **kwargs:Â Any) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a 4D array containing all images from a compressed video file.</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; video = read_video('/tmp/river.mp4')
&gt;&gt;&gt; show_video(video)
&gt;&gt;&gt; url = 'https://github.com/hhoppe/data/raw/main/video.mp4'
&gt;&gt;&gt; show_video(read_video(url))
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path_or_url</code></strong></dt>
<dd>Input video file.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional parameters for <code><a title="mediapy.VideoReader" href="#mediapy.VideoReader">VideoReader</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A 4D array with dimensions (frame, height, width, channel).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_video(path_or_url: _StrOrPath, **kwargs: Any) -&gt; np.ndarray:
  &#34;&#34;&#34;Returns a 4D array containing all images from a compressed video file.

  &gt;&gt;&gt; video = read_video(&#39;/tmp/river.mp4&#39;)
  &gt;&gt;&gt; show_video(video)
  &gt;&gt;&gt; url = &#39;https://github.com/hhoppe/data/raw/main/video.mp4&#39;
  &gt;&gt;&gt; show_video(read_video(url))

  Args:
    path_or_url: Input video file.
    **kwargs: Additional parameters for `VideoReader`.

  Returns:
    A 4D array with dimensions (frame, height, width, channel).
  &#34;&#34;&#34;
  with VideoReader(path_or_url, **kwargs) as reader:
    return np.array(tuple(reader))</code></pre>
</details>
</dd>
<dt id="mediapy.resize_image"><code class="name flex">
<span>def <span class="ident">resize_image</span></span>(<span>image:Â Any, shape:Â Tuple[int,Â int]) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Resizes image to specified spatial dimensions using a Lanczos filter.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>image</code></strong></dt>
<dd>Array-like 2D or 3D object, where dtype is uint or floating-point.</dd>
<dt><strong><code>shape</code></strong></dt>
<dd>2D spatial dimensions (height, width) of output image.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A resampled image whose spatial dimensions match <code>shape</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resize_image(image: Any, shape: Tuple[int, int]) -&gt; np.ndarray:
  &#34;&#34;&#34;Resizes image to specified spatial dimensions using a Lanczos filter.

  Args:
    image: Array-like 2D or 3D object, where dtype is uint or floating-point.
    shape: 2D spatial dimensions (height, width) of output image.

  Returns:
    A resampled image whose spatial dimensions match `shape`.
  &#34;&#34;&#34;
  image = _as_valid_media_array(image)
  if image.ndim not in (2, 3):
    raise ValueError(f&#39;Image shape {image.shape} is neither 2D nor 3D.&#39;)
  _check_2d_shape(shape)

  # A PIL image can be multichannel only if it has 3 or 4 uint8 channels,
  # and it can be resized only if it is uint8 or float32.
  supported_single_channel = ((issubclass(image.dtype.type, np.floating) or
                               image.dtype == np.uint8) and image.ndim == 2)
  supported_multichannel = (
      image.dtype == np.uint8 and image.ndim == 3 and image.shape[2] in (3, 4))
  if supported_single_channel or supported_multichannel:
    return np.array(
        _pil_image(image).resize(shape[::-1], resample=PIL.Image.LANCZOS),
        dtype=image.dtype)
  if image.ndim == 2:
    # We convert to floating-poing for resizing and convert back.
    return to_type(resize_image(to_float01(image), shape), image.dtype)
  # We resize each image channel individually.
  return np.dstack(
      [resize_image(channel, shape) for channel in np.moveaxis(image, -1, 0)])</code></pre>
</details>
</dd>
<dt id="mediapy.resize_video"><code class="name flex">
<span>def <span class="ident">resize_video</span></span>(<span>video:Â Iterable[numpy.ndarray], shape:Â Tuple[int,Â int]) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Resizes <code>video</code> to specified spatial dimensions using a Lanczos filter.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>video</code></strong></dt>
<dd>Iterable of images.</dd>
<dt><strong><code>shape</code></strong></dt>
<dd>2D spatial dimensions (height, width) of output video.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A resampled video whose spatial dimensions match <code>shape</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resize_video(video: Iterable[np.ndarray], shape: Tuple[int,
                                                           int]) -&gt; np.ndarray:
  &#34;&#34;&#34;Resizes `video` to specified spatial dimensions using a Lanczos filter.

  Args:
    video: Iterable of images.
    shape: 2D spatial dimensions (height, width) of output video.

  Returns:
    A resampled video whose spatial dimensions match `shape`.
  &#34;&#34;&#34;
  _check_2d_shape(shape)
  return np.array([resize_image(image, shape) for image in video])</code></pre>
</details>
</dd>
<dt id="mediapy.rgb_from_ycbcr"><code class="name flex">
<span>def <span class="ident">rgb_from_ycbcr</span></span>(<span>ycbcr:Â Any) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the YCbCr image/video mapped to RGB [0,1] color space.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rgb_from_ycbcr(ycbcr: Any) -&gt; np.ndarray:
  &#34;&#34;&#34;Returns the YCbCr image/video mapped to RGB [0,1] color space.&#34;&#34;&#34;
  ycbcr = to_float01(ycbcr)
  if ycbcr.shape[-1] != 3:
    raise ValueError(f&#39;The last dimension in {ycbcr.shape} is not 3.&#39;)
  return np.matmul(ycbcr * 255.0 - _YCBCR_OFFSET, _RGB_FROM_YCBCR_MATRIX)</code></pre>
</details>
</dd>
<dt id="mediapy.rgb_from_yuv"><code class="name flex">
<span>def <span class="ident">rgb_from_yuv</span></span>(<span>yuv:Â Any) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the YUV image/video mapped to RGB [0,1] color space.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rgb_from_yuv(yuv: Any) -&gt; np.ndarray:
  &#34;&#34;&#34;Returns the YUV image/video mapped to RGB [0,1] color space.&#34;&#34;&#34;
  yuv = to_float01(yuv)
  if yuv.shape[-1] != 3:
    raise ValueError(f&#39;The last dimension in {yuv.shape} is not 3.&#39;)
  return np.matmul(yuv - _YUV_CHROMA_OFFSET, _RGB_FROM_YUV_MATRIX)</code></pre>
</details>
</dd>
<dt id="mediapy.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>args:Â Union[str,Â Sequence[str]]) â€‘>Â NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Executes command, printing output from stdout and stderr.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>args</code></strong></dt>
<dd>Command to execute, which can be either a string or a sequence of word
strings, as in <code>subprocess.run()</code>.
If <code>args</code> is a string, the shell is
invoked to interpret it.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>RuntimeError</code></dt>
<dd>If the command's exit code is nonzero.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(args: Union[str, Sequence[str]]) -&gt; None:
  &#34;&#34;&#34;Executes command, printing output from stdout and stderr.

  Args:
    args: Command to execute, which can be either a string or a sequence of word
      strings, as in `subprocess.run()`.  If `args` is a string, the shell is
      invoked to interpret it.

  Raises:
    RuntimeError: If the command&#39;s exit code is nonzero.
  &#34;&#34;&#34;
  proc = subprocess.run(
      args,
      shell=isinstance(args, str),
      stdout=subprocess.PIPE,
      stderr=subprocess.STDOUT,
      check=False,
      universal_newlines=True)
  print(proc.stdout, end=&#39;&#39;, flush=True)
  if proc.returncode:
    raise RuntimeError(
        f&#34;Command &#39;{proc.args}&#39; failed with code {proc.returncode}.&#34;)</code></pre>
</details>
</dd>
<dt id="mediapy.set_max_output_height"><code class="name flex">
<span>def <span class="ident">set_max_output_height</span></span>(<span>num_pixels:Â int) â€‘>Â NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the maximum height of the current output cell, if using Colab.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_max_output_height(num_pixels: int) -&gt; None:
  &#34;&#34;&#34;Sets the maximum height of the current output cell, if using Colab.&#34;&#34;&#34;
  try:
    # We want to fail gracefully for non-Colab IPython notebooks.
    output = importlib.import_module(&#39;google.colab.output&#39;)
    s = (&#39;google.colab.output.setIframeHeight(&#39;
         f&#39;0, true, {{maxHeight: {num_pixels}}})&#39;)
    output.eval_js(s)  # type: ignore
  except ModuleNotFoundError:
    pass</code></pre>
</details>
</dd>
<dt id="mediapy.set_output_height"><code class="name flex">
<span>def <span class="ident">set_output_height</span></span>(<span>num_pixels:Â int) â€‘>Â NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Overrides the height of the current output cell, if using Colab.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_output_height(num_pixels: int) -&gt; None:
  &#34;&#34;&#34;Overrides the height of the current output cell, if using Colab.&#34;&#34;&#34;
  try:
    # We want to fail gracefully for non-Colab IPython notebooks.
    output = importlib.import_module(&#39;google.colab.output&#39;)
    s = f&#39;google.colab.output.setIframeHeight(&#34;{num_pixels}px&#34;)&#39;
    output.eval_js(s)  # type: ignore
  except ModuleNotFoundError:
    pass</code></pre>
</details>
</dd>
<dt id="mediapy.show_image"><code class="name flex">
<span>def <span class="ident">show_image</span></span>(<span>image:Â Any, *, title:Â Union[str,Â NoneType]Â =Â None, **kwargs:Â Any) â€‘>Â NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Displays an image in the notebook and optionally saves it to a file.</p>
<p>See <code><a title="mediapy.show_images" href="#mediapy.show_images">show_images()</a></code>.</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; show_image(np.random.rand(100, 100))
&gt;&gt;&gt; show_image(np.random.randint(0, 256, size=(80, 80, 3), dtype='uint8'))
&gt;&gt;&gt; show_image(np.random.rand(10, 10) - 0.5, cmap='bwr', height=100)
&gt;&gt;&gt; show_image(read_image('/tmp/image.png'))
&gt;&gt;&gt; url = 'https://github.com/hhoppe/data/raw/main/image.png'
&gt;&gt;&gt; show_image(read_image(url))
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>image</code></strong></dt>
<dd>2D array-like, or 3D array-like with 1, 3, or 4 channels.</dd>
<dt><strong><code>title</code></strong></dt>
<dd>Optional text shown centered above the image.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>See <code><a title="mediapy.show_images" href="#mediapy.show_images">show_images()</a></code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def show_image(image: Any,
               *,
               title: Optional[str] = None,
               **kwargs: Any) -&gt; None:
  &#34;&#34;&#34;Displays an image in the notebook and optionally saves it to a file.

  See `show_images`.

  &gt;&gt;&gt; show_image(np.random.rand(100, 100))
  &gt;&gt;&gt; show_image(np.random.randint(0, 256, size=(80, 80, 3), dtype=&#39;uint8&#39;))
  &gt;&gt;&gt; show_image(np.random.rand(10, 10) - 0.5, cmap=&#39;bwr&#39;, height=100)
  &gt;&gt;&gt; show_image(read_image(&#39;/tmp/image.png&#39;))
  &gt;&gt;&gt; url = &#39;https://github.com/hhoppe/data/raw/main/image.png&#39;
  &gt;&gt;&gt; show_image(read_image(url))

  Args:
    image: 2D array-like, or 3D array-like with 1, 3, or 4 channels.
    title: Optional text shown centered above the image.
    **kwargs: See `show_images`.
  &#34;&#34;&#34;
  show_images([image], [title], **kwargs)</code></pre>
</details>
</dd>
<dt id="mediapy.show_images"><code class="name flex">
<span>def <span class="ident">show_images</span></span>(<span>images:Â Union[Iterable[numpy.ndarray],Â Mapping[str,Â numpy.ndarray]], titles:Â Union[Iterable[Union[str,Â NoneType]],Â NoneType]Â =Â None, *, width:Â Union[int,Â NoneType]Â =Â None, height:Â Union[int,Â NoneType]Â =Â None, downsample:Â boolÂ =Â True, columns:Â Union[int,Â NoneType]Â =Â None, vmin:Â Union[float,Â NoneType]Â =Â None, vmax:Â Union[float,Â NoneType]Â =Â None, cmap:Â Union[str,Â Callable[[numpy.ndarray],Â numpy.ndarray]]Â =Â 'gray', border:Â Union[bool,Â str]Â =Â False) â€‘>Â NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Displays a row of images in the IPython/Jupyter notebook.</p>
<p>If <code>show_save.dir</code> is not None, saves each titled image to a file based
on the title.</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; image1, image2 = np.random.rand(64, 64, 3), color_ramp((64, 64))
&gt;&gt;&gt; show_images([image1, image2])
&gt;&gt;&gt; show_images({'random image': image1, 'color ramp': image2}, height=128)
&gt;&gt;&gt; show_images([image1, image2] * 5, columns=4, border=True)
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>images</code></strong></dt>
<dd>Iterable of images, or dictionary of <code>{title: image}</code>.
Each image
must be either a 2D array or a 3D array with 1, 3, or 4 channels.</dd>
<dt><strong><code>titles</code></strong></dt>
<dd>Optional strings shown above the corresponding images.</dd>
<dt><strong><code>width</code></strong></dt>
<dd>Optional, overrides displayed width (in pixels).</dd>
<dt><strong><code>height</code></strong></dt>
<dd>Optional, overrides displayed height (in pixels).</dd>
<dt><strong><code>downsample</code></strong></dt>
<dd>If True, each image whose width or height is greater than the
specified <code>height</code> or <code>width</code> is resampled to the display resolution. This
improves antialiasing and reduces the size of the notebook.</dd>
<dt><strong><code>columns</code></strong></dt>
<dd>Optional, maximum number of images per row.</dd>
<dt><strong><code>vmin</code></strong></dt>
<dd>For single-channel image, explicit min value for display.</dd>
<dt><strong><code>vmax</code></strong></dt>
<dd>For single-channel image, explicit max value for display.</dd>
<dt><strong><code>cmap</code></strong></dt>
<dd>For single-channel image, pyplot color map or callable to map 1D to 3D
color.</dd>
<dt><strong><code>border</code></strong></dt>
<dd>If <code>bool</code>, whether to place a black boundary around the image, or if
<code>str</code>, the boundary CSS style.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def show_images(
    images: Union[Iterable[np.ndarray], Mapping[str, np.ndarray]],
    titles: Optional[Iterable[Optional[str]]] = None,
    *,
    width: Optional[int] = None,
    height: Optional[int] = None,
    downsample: bool = True,
    columns: Optional[int] = None,
    vmin: Optional[float] = None,
    vmax: Optional[float] = None,
    cmap: Union[str, Callable[[np.ndarray], np.ndarray]] = &#39;gray&#39;,
    border: Union[bool, str] = False,
) -&gt; None:
  &#34;&#34;&#34;Displays a row of images in the IPython/Jupyter notebook.

  If ```show_save.dir``` is not None, saves each titled image to a file based
  on the title.

  &gt;&gt;&gt; image1, image2 = np.random.rand(64, 64, 3), color_ramp((64, 64))
  &gt;&gt;&gt; show_images([image1, image2])
  &gt;&gt;&gt; show_images({&#39;random image&#39;: image1, &#39;color ramp&#39;: image2}, height=128)
  &gt;&gt;&gt; show_images([image1, image2] * 5, columns=4, border=True)

  Args:
    images: Iterable of images, or dictionary of `{title: image}`.  Each image
      must be either a 2D array or a 3D array with 1, 3, or 4 channels.
    titles: Optional strings shown above the corresponding images.
    width: Optional, overrides displayed width (in pixels).
    height: Optional, overrides displayed height (in pixels).
    downsample: If True, each image whose width or height is greater than the
      specified `height` or `width` is resampled to the display resolution. This
      improves antialiasing and reduces the size of the notebook.
    columns: Optional, maximum number of images per row.
    vmin: For single-channel image, explicit min value for display.
    vmax: For single-channel image, explicit max value for display.
    cmap: For single-channel image, pyplot color map or callable to map 1D to 3D
      color.
    border: If `bool`, whether to place a black boundary around the image, or if
      `str`, the boundary CSS style.
  &#34;&#34;&#34;
  if isinstance(images, collections.abc.Mapping):
    if titles is not None:
      raise ValueError(&#39;Cannot have images dictionary and titles parameter.&#39;)
    list_titles, list_images = list(images.keys()), list(images.values())
  else:
    list_images: List[np.ndarray] = list(images)  # type: ignore
    if titles is None:
      list_titles = [None] * len(list_images)
    else:
      list_titles = list(titles)
      if len(list_images) != len(list_titles):
        raise ValueError(&#39;Number of images does not match number of titles&#39;
                         f&#39; ({len(list_images)} vs {len(list_titles)}).&#39;)

  def ensure_mapped_to_rgb(image: Any) -&gt; np.ndarray:
    image = _as_valid_media_array(image)
    if not (image.ndim == 2 or
            (image.ndim == 3 and image.shape[2] in (1, 3, 4))):
      raise ValueError(f&#39;Image with shape {image.shape} is neither a 2D array&#39;
                       &#39; nor a 3D array with 1, 3, or 4 channels.&#39;)
    if image.ndim == 3 and image.shape[2] == 1:
      image = image[:, :, 0]
    if image.ndim == 2:
      image = to_rgb(image, vmin=vmin, vmax=vmax, cmap=cmap)
    return image

  list_images = [ensure_mapped_to_rgb(image) for image in list_images]

  def maybe_downsample(image: np.ndarray) -&gt; np.ndarray:
    w, h = _get_width_height(width, height, image.shape[:2])
    if w &lt; image.shape[1] or h &lt; image.shape[0]:
      image = resize_image(image, (h, w))
    return image

  if downsample:
    list_images = [maybe_downsample(image) for image in list_images]
  png_datas = [compress_image(to_uint8(image)) for image in list_images]

  for title, png_data in zip(list_titles, png_datas):
    if title and show_save.dir:
      path = pathlib.Path(show_save.dir) / f&#39;{title}.png&#39;
      with _open(path, mode=&#39;wb&#39;) as f:
        f.write(png_data)

  def html_from_compressed_images() -&gt; str:
    html_strings = []
    for image, title, png_data in zip(list_images, list_titles, png_datas):
      w, h = _get_width_height(width, height, image.shape[:2])
      html_strings.append(
          html_from_compressed_image(
              png_data, w, h, title=title, border=border))
    # Create single-row tables each with no more than &#39;columns&#39; elements.
    table_strings = []
    for row_html_strings in _chunked(html_strings, columns):
      s = &#39;&#39;.join(f&#39;&lt;td&gt;{e}&lt;/td&gt;&#39; for e in row_html_strings)
      table_strings.append(
          f&#39;&lt;table style=&#34;border-spacing:0;&#34;&gt;&lt;tr&gt;{s}&lt;/tr&gt;&lt;/table&gt;&#39;)
    return &#39;&#39;.join(table_strings)

  s = html_from_compressed_images()
  while len(s) &gt; _IPYTHON_HTML_SIZE_LIMIT * 0.5:
    list_images = [image[::2, ::2] for image in list_images]
    png_datas = [compress_image(to_uint8(image)) for image in list_images]
    s = html_from_compressed_images()
  IPython.display.display(IPython.display.HTML(s))</code></pre>
</details>
</dd>
<dt id="mediapy.show_video"><code class="name flex">
<span>def <span class="ident">show_video</span></span>(<span>images:Â Iterable[numpy.ndarray], *, title:Â Union[str,Â NoneType]Â =Â None, **kwargs:Â Any) â€‘>Â NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Displays a video in the IPython notebook and optionally saves it to a file.</p>
<p>See <code><a title="mediapy.show_videos" href="#mediapy.show_videos">show_videos()</a></code>.</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; video = read_video('https://github.com/hhoppe/data/raw/main/video.mp4')
&gt;&gt;&gt; show_video(video, title='River video', fps=10.0)
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; show_video(moving_circle((80, 80), num_images=10), border=True)
&gt;&gt;&gt; show_video(read_video('/tmp/river.mp4'))
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>images</code></strong></dt>
<dd>Iterable of video frames.</dd>
<dt><strong><code>title</code></strong></dt>
<dd>Optional text shown centered above the video.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>See <code><a title="mediapy.show_videos" href="#mediapy.show_videos">show_videos()</a></code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def show_video(images: Iterable[np.ndarray],
               *,
               title: Optional[str] = None,
               **kwargs: Any) -&gt; None:
  &#34;&#34;&#34;Displays a video in the IPython notebook and optionally saves it to a file.

  See `show_videos`.

  &gt;&gt;&gt; video = read_video(&#39;https://github.com/hhoppe/data/raw/main/video.mp4&#39;)
  &gt;&gt;&gt; show_video(video, title=&#39;River video&#39;, fps=10.0)

  &gt;&gt;&gt; show_video(moving_circle((80, 80), num_images=10), border=True)
  &gt;&gt;&gt; show_video(read_video(&#39;/tmp/river.mp4&#39;))

  Args:
    images: Iterable of video frames.
    title: Optional text shown centered above the video.
    **kwargs: See `show_videos`.
  &#34;&#34;&#34;
  show_videos([images], [title], **kwargs)</code></pre>
</details>
</dd>
<dt id="mediapy.show_videos"><code class="name flex">
<span>def <span class="ident">show_videos</span></span>(<span>videos:Â Union[Iterable[Iterable[numpy.ndarray]],Â Mapping[str,Â Iterable[numpy.ndarray]]], titles:Â Union[Sequence[Union[str,Â NoneType]],Â NoneType]Â =Â None, *, width:Â Union[int,Â NoneType]Â =Â None, height:Â Union[int,Â NoneType]Â =Â None, downsample:Â boolÂ =Â True, columns:Â Union[int,Â NoneType]Â =Â None, fps:Â Union[float,Â NoneType]Â =Â None, bps:Â Union[int,Â NoneType]Â =Â None, qp:Â Union[int,Â NoneType]Â =Â None, codec:Â strÂ =Â 'h264', **kwargs:Â Any) â€‘>Â NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Displays a row of videos in the IPython notebook.</p>
<p>Creates HTML with <code>&lt;video&gt;</code> tags containing embedded H264-encoded bytestrings.
If <code>codec</code> is set to 'gif', we instead use <code>&lt;img&gt;</code> tags containing embedded
GIF-encoded bytestrings.
Note that the resulting GIF animations skip frames
when the <code>fps</code> period is not a multiple of 10 ms units (GIF frame delay
units).
Encoding at <code>fps</code> = 20.0, 25.0, or 50.0 works fine.</p>
<p>If <code>show_save.dir</code> is not None, saves each titled video to a file based
on the title.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>videos</code></strong></dt>
<dd>Iterable of videos, or dictionary of <code>{title: video}</code>.
Each video
must be an iterable of images.</dd>
<dt><strong><code>titles</code></strong></dt>
<dd>Optional sequence of strings shown above the corresponding videos.</dd>
<dt><strong><code>width</code></strong></dt>
<dd>Optional, overrides displayed width (in pixels).</dd>
<dt><strong><code>height</code></strong></dt>
<dd>Optional, overrides displayed height (in pixels).</dd>
<dt><strong><code>downsample</code></strong></dt>
<dd>If True, each video whose width or height is greater than the
specified <code>height</code> or <code>width</code> is resampled to the display resolution. This
improves antialiasing and reduces the size of the notebook.</dd>
<dt><strong><code>columns</code></strong></dt>
<dd>Optional, maximum number of videos per row.</dd>
<dt><strong><code>fps</code></strong></dt>
<dd>Frames-per-second frame rate (default is 60.0 except 25.0 for GIF).</dd>
<dt><strong><code>bps</code></strong></dt>
<dd>Bits-per-second bitrate (default None).</dd>
<dt><strong><code>qp</code></strong></dt>
<dd>Quantization parameter for video compression quality (default None).</dd>
<dt><strong><code>codec</code></strong></dt>
<dd>Compression algorithm; must be either 'h264' or 'gif'.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional parameters (<code>border</code>, <code>loop</code>, <code>autoplay</code>) for
<code><a title="mediapy.html_from_compressed_video" href="#mediapy.html_from_compressed_video">html_from_compressed_video()</a></code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def show_videos(videos: Union[Iterable[Iterable[np.ndarray]],
                              Mapping[str, Iterable[np.ndarray]]],
                titles: Optional[Sequence[Optional[str]]] = None,
                *,
                width: Optional[int] = None,
                height: Optional[int] = None,
                downsample: bool = True,
                columns: Optional[int] = None,
                fps: Optional[float] = None,
                bps: Optional[int] = None,
                qp: Optional[int] = None,
                codec: str = &#39;h264&#39;,
                **kwargs: Any) -&gt; None:
  &#34;&#34;&#34;Displays a row of videos in the IPython notebook.

  Creates HTML with `&lt;video&gt;` tags containing embedded H264-encoded bytestrings.
  If `codec` is set to &#39;gif&#39;, we instead use `&lt;img&gt;` tags containing embedded
  GIF-encoded bytestrings.  Note that the resulting GIF animations skip frames
  when the `fps` period is not a multiple of 10 ms units (GIF frame delay
  units).  Encoding at `fps` = 20.0, 25.0, or 50.0 works fine.

  If ```show_save.dir``` is not None, saves each titled video to a file based
  on the title.

  Args:
    videos: Iterable of videos, or dictionary of `{title: video}`.  Each video
      must be an iterable of images.
    titles: Optional sequence of strings shown above the corresponding videos.
    width: Optional, overrides displayed width (in pixels).
    height: Optional, overrides displayed height (in pixels).
    downsample: If True, each video whose width or height is greater than the
      specified `height` or `width` is resampled to the display resolution. This
      improves antialiasing and reduces the size of the notebook.
    columns: Optional, maximum number of videos per row.
    fps: Frames-per-second frame rate (default is 60.0 except 25.0 for GIF).
    bps: Bits-per-second bitrate (default None).
    qp: Quantization parameter for video compression quality (default None).
    codec: Compression algorithm; must be either &#39;h264&#39; or &#39;gif&#39;.
    **kwargs: Additional parameters (`border`, `loop`, `autoplay`) for
      `html_from_compressed_video`.
  &#34;&#34;&#34;
  if isinstance(videos, collections.abc.Mapping):
    if titles is not None:
      raise ValueError(
          &#39;Cannot have both a video dictionary and a titles parameter.&#39;)
    list_titles, list_videos = list(videos.keys()), list(videos.values())
  else:
    list_videos: List[Iterable[np.ndarray]] = list(videos)  # type: ignore
    list_titles = [None] * len(list_videos) if titles is None else list(titles)
    if len(list_videos) != len(list_titles):
      raise ValueError(&#39;Number of videos does not match number of titles&#39;
                       f&#39; ({len(list_videos)} vs {len(list_titles)}).&#39;)
  if codec not in (&#39;h264&#39;, &#39;gif&#39;):
    raise ValueError(f&#39;Codec {codec} is neither h264 or gif.&#39;)

  html_strings = []
  for video, title in zip(list_videos, list_titles):
    first_image, video = peek_first(video)
    w, h = _get_width_height(width, height, first_image.shape[:2])
    if downsample and (w &lt; first_image.shape[1] or h &lt; first_image.shape[0]):
      # Not resize_video() because each image may have different depth and type.
      video = [resize_image(image, (h, w)) for image in video]
    data = compress_video(video, fps=fps, bps=bps, qp=qp, codec=codec)
    if title and show_save.dir:
      suffix = _filename_suffix_from_codec(codec)
      path = pathlib.Path(show_save.dir) / f&#39;{title}{suffix}&#39;
      with _open(path, mode=&#39;wb&#39;) as f:
        f.write(data)
    if codec == &#39;gif&#39;:
      html_string = html_from_compressed_image(
          data, w, h, title=title, fmt=&#39;gif&#39;, **kwargs)
    else:
      html_string = html_from_compressed_video(
          data, w, h, title=title, **kwargs)
    html_strings.append(html_string)

  # Create single-row tables each with no more than &#39;columns&#39; elements.
  table_strings = []
  for row_html_strings in _chunked(html_strings, columns):
    s = &#39;&#39;.join(f&#39;&lt;td&gt;{e}&lt;/td&gt;&#39; for e in row_html_strings)
    table_strings.append(
        f&#39;&lt;table style=&#34;border-spacing:0;&#34;&gt;&lt;tr&gt;{s}&lt;/tr&gt;&lt;/table&gt;&#39;)
  s = &#39;&#39;.join(table_strings)
  IPython.display.display(IPython.display.HTML(s))</code></pre>
</details>
</dd>
<dt id="mediapy.to_float01"><code class="name flex">
<span>def <span class="ident">to_float01</span></span>(<span>a:Â Any, dtype:Â AnyÂ =Â numpy.float32) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>If array has unsigned integers, rescales them to the range [0.0, 1.0].</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>a</code></strong></dt>
<dd>Input array.</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>Desired floating-point type if rescaling occurs.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A new array of dtype values in the range [0.0, 1.0] if the input array <code>a</code>
contains unsigned integers; otherwise, array <code>a</code> is returned unchanged.
Scaling is such that uint(0) maps to 0.0 and uint(MAX) maps to 1.0.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_float01(a: Any, dtype: Any = np.float32) -&gt; np.ndarray:
  &#34;&#34;&#34;If array has unsigned integers, rescales them to the range [0.0, 1.0].

  Args:
    a: Input array.
    dtype: Desired floating-point type if rescaling occurs.

  Returns:
    A new array of dtype values in the range [0.0, 1.0] if the input array `a`
    contains unsigned integers; otherwise, array `a` is returned unchanged.

  Scaling is such that uint(0) maps to 0.0 and uint(MAX) maps to 1.0.
  &#34;&#34;&#34;
  dtype = np.dtype(dtype)
  if not issubclass(dtype.type, np.floating):
    raise ValueError(f&#39;Type {dtype} is not floating-point.&#39;)
  a = np.asarray(a)
  if issubclass(a.dtype.type, np.floating):
    return a
  return to_type(a, dtype)</code></pre>
</details>
</dd>
<dt id="mediapy.to_rgb"><code class="name flex">
<span>def <span class="ident">to_rgb</span></span>(<span>array:Â Any, *, vmin:Â Union[float,Â NoneType]Â =Â None, vmax:Â Union[float,Â NoneType]Â =Â None, cmap:Â Union[str,Â Callable[[numpy.ndarray],Â numpy.ndarray]]Â =Â 'gray') â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Maps scalar values to RGB using value bounds and a color map.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>array</code></strong></dt>
<dd>Scalar values, with arbitrary shape.</dd>
<dt><strong><code>vmin</code></strong></dt>
<dd>Explicit min value for remapping; if None, it is obtained as the
minimum finite value of <code>array</code>.</dd>
<dt><strong><code>vmax</code></strong></dt>
<dd>Explicit max value for remapping; if None, it is obtained as the
maximum finite value of <code>array</code>.</dd>
<dt><strong><code>cmap</code></strong></dt>
<dd>A pyplot color map or callable, to map from 1D value to 3D or 4D
color.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A new array in which each element is affinely mapped from [vmin, vmax]
to [0.0, 1.0] and then color-mapped.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_rgb(
    array: Any,
    *,
    vmin: Optional[float] = None,
    vmax: Optional[float] = None,
    cmap: Union[str, Callable[[np.ndarray], np.ndarray]] = &#39;gray&#39;,
) -&gt; np.ndarray:
  &#34;&#34;&#34;Maps scalar values to RGB using value bounds and a color map.

  Args:
    array: Scalar values, with arbitrary shape.
    vmin: Explicit min value for remapping; if None, it is obtained as the
      minimum finite value of `array`.
    vmax: Explicit max value for remapping; if None, it is obtained as the
      maximum finite value of `array`.
    cmap: A pyplot color map or callable, to map from 1D value to 3D or 4D
      color.

  Returns:
    A new array in which each element is affinely mapped from [vmin, vmax]
    to [0.0, 1.0] and then color-mapped.
  &#34;&#34;&#34;
  array = _as_valid_media_array(array)
  # For future numpy version 1.7.0:
  # vmin = np.min(array, where=np.isfinite(array)) if vmin is None else vmin
  # vmax = np.max(array, where=np.isfinite(array)) if vmax is None else vmax
  vmin = np.min(np.where(np.isfinite(array), array,
                         np.inf)) if vmin is None else vmin
  vmax = np.max(np.where(np.isfinite(array), array,
                         -np.inf)) if vmax is None else vmax
  array = (array - vmin) / (vmax - vmin + np.finfo(float).eps)
  if isinstance(cmap, str):
    rgb_from_scalar = plt.cm.get_cmap(cmap)
  else:
    rgb_from_scalar = cmap
  array = rgb_from_scalar(array)
  # If there is a fully opaque alpha channel, remove it.
  if (array.shape[-1] == 4 and np.all(to_float01(array[..., 3])) == 1.0):
    array = array[..., :3]
  return array</code></pre>
</details>
</dd>
<dt id="mediapy.to_type"><code class="name flex">
<span>def <span class="ident">to_type</span></span>(<span>a:Â Any, dtype:Â Any) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns media array converted to specified type.</p>
<p>A "media array" is one in which the dtype is either a floating-point type
(np.float32 or np.float64) or an unsigned integer type.
The array values are
assumed to lie in the range [0.0, 1.0] for floating-point values, and in the
full range for unsigned integers, e.g. [0, 255] for np.uint8.
Conversion
between integers and floats maps uint(0) to 0.0 and uint(MAX) to 1.0.
The
input array may also be of type bool, whereby True maps to uint(MAX) or 1.0.
The values are scaled and clamped as appropriate during type conversions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>a</code></strong></dt>
<dd>Input array-like object (of type floating-point, unsigned int, or bool).</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>Desired output type (floating-point or unsigned int).</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Array <code>a</code> if it is already of the specified dtype, else a converted array.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_type(a: Any, dtype: Any) -&gt; np.ndarray:
  &#34;&#34;&#34;Returns media array converted to specified type.

  A &#34;media array&#34; is one in which the dtype is either a floating-point type
  (np.float32 or np.float64) or an unsigned integer type.  The array values are
  assumed to lie in the range [0.0, 1.0] for floating-point values, and in the
  full range for unsigned integers, e.g. [0, 255] for np.uint8.  Conversion
  between integers and floats maps uint(0) to 0.0 and uint(MAX) to 1.0.  The
  input array may also be of type bool, whereby True maps to uint(MAX) or 1.0.
  The values are scaled and clamped as appropriate during type conversions.

  Args:
    a: Input array-like object (of type floating-point, unsigned int, or bool).
    dtype: Desired output type (floating-point or unsigned int).

  Returns:
    Array `a` if it is already of the specified dtype, else a converted array.
  &#34;&#34;&#34;
  a = np.asarray(a)
  dtype = _as_valid_media_type(dtype)
  if a.dtype != bool:
    _as_valid_media_type(a.dtype)  # Verify that &#39;a&#39; has a valid dtype.
  if a.dtype == bool:
    result = a.astype(dtype)
    if issubclass(dtype.type, np.unsignedinteger):
      result = result * dtype.type(np.iinfo(dtype).max)
  elif a.dtype == dtype:
    result = a
  elif issubclass(dtype.type, np.unsignedinteger):
    if issubclass(a.dtype.type, np.unsignedinteger):
      src_max = np.iinfo(a.dtype).max
    else:
      a = np.clip(a, 0.0, 1.0)
      src_max = 1.0
    dst_max = np.iinfo(dtype).max
    if dst_max &lt;= np.iinfo(np.uint16).max:
      result = (a * np.float32(dst_max / src_max) + 0.5).astype(dtype)
    elif dst_max &lt;= np.iinfo(np.uint32).max:
      result = (a.astype(np.float64) * (dst_max / src_max) + 0.5).astype(dtype)
    else:
      # https://stackoverflow.com/a/66306123/
      a = a.astype(np.float64) * (dst_max / src_max) + 0.5
      dst = np.atleast_1d(a)
      values_too_large = dst &gt;= np.float64(dst_max)
      dst = dst.astype(dtype)
      dst[values_too_large] = dst_max
      result = dst if a.ndim &gt; 0 else dst[0]
  else:
    assert issubclass(dtype.type, np.floating)
    result = a.astype(dtype)
    if issubclass(a.dtype.type, np.unsignedinteger):
      result = result / dtype.type(np.iinfo(a.dtype).max)
  return result</code></pre>
</details>
</dd>
<dt id="mediapy.to_uint"><code class="name flex">
<span>def <span class="ident">to_uint</span></span>(<span>a:Â Any, dtype:Â Any) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns array converted to unsigned-integer dtype; see <code><a title="mediapy.to_type" href="#mediapy.to_type">to_type()</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_uint(a: Any, dtype: Any) -&gt; np.ndarray:
  &#34;&#34;&#34;Returns array converted to unsigned-integer dtype; see `to_type`.&#34;&#34;&#34;
  dtype = np.dtype(dtype)
  if not issubclass(dtype.type, np.unsignedinteger):
    raise ValueError(f&#39;Type {dtype} is not an unsigned integer.&#39;)
  return to_type(a, dtype)</code></pre>
</details>
</dd>
<dt id="mediapy.to_uint8"><code class="name flex">
<span>def <span class="ident">to_uint8</span></span>(<span>a:Â Any) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns array converted to uint8 values; see <code><a title="mediapy.to_type" href="#mediapy.to_type">to_type()</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_uint8(a: Any) -&gt; np.ndarray:
  &#34;&#34;&#34;Returns array converted to uint8 values; see `to_type`.&#34;&#34;&#34;
  return to_type(a, np.uint8)</code></pre>
</details>
</dd>
<dt id="mediapy.video_is_available"><code class="name flex">
<span>def <span class="ident">video_is_available</span></span>(<span>) â€‘>Â bool</span>
</code></dt>
<dd>
<div class="desc"><p>Returns True if the program <code>FFMPEG_NAME</code> is found.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def video_is_available() -&gt; bool:
  &#34;&#34;&#34;Returns True if the program `FFMPEG_NAME` is found.&#34;&#34;&#34;
  return _search_for_ffmpeg_path() is not None</code></pre>
</details>
</dd>
<dt id="mediapy.write_image"><code class="name flex">
<span>def <span class="ident">write_image</span></span>(<span>path:Â Union[str,Â ForwardRef('os.PathLike[str]')], image:Â numpy.ndarray, **kwargs:Â Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Writes an image to a file.</p>
<p>Encoding is performed using <code>PIL</code>, which supports <code>uint8</code> images with 1, 3,
or 4 channels and <code>uint16</code> images with a single channel.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong></dt>
<dd>Path of output file.</dd>
<dt><strong><code>image</code></strong></dt>
<dd>Array-like object.
If its type is float, it is converted to np.uint8
using <code><a title="mediapy.to_uint8" href="#mediapy.to_uint8">to_uint8()</a></code> (thus clamping to the input to the range [0.0, 1.0]).
Otherwise it must be np.uint8 or np.uint16.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional parameters for <code>PIL.Image.save()</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_image(path: _StrOrPath, image: np.ndarray, **kwargs: Any) -&gt; None:
  &#34;&#34;&#34;Writes an image to a file.

  Encoding is performed using `PIL`, which supports `uint8` images with 1, 3,
  or 4 channels and `uint16` images with a single channel.

  Args:
    path: Path of output file.
    image: Array-like object.  If its type is float, it is converted to np.uint8
      using `to_uint8` (thus clamping to the input to the range [0.0, 1.0]).
      Otherwise it must be np.uint8 or np.uint16.
    **kwargs: Additional parameters for `PIL.Image.save()`.
  &#34;&#34;&#34;
  if issubclass(image.dtype.type, np.floating):
    image = to_uint8(image)
  with _open(path, &#39;wb&#39;) as f:
    _pil_image(image).save(f, format=&#39;png&#39;, **kwargs)</code></pre>
</details>
</dd>
<dt id="mediapy.write_via_local_file"><code class="name flex">
<span>def <span class="ident">write_via_local_file</span></span>(<span>path:Â Union[str,Â ForwardRef('os.PathLike[str]')]) â€‘>Â Generator[str,Â NoneType,Â NoneType]</span>
</code></dt>
<dd>
<div class="desc"><p>Context to write a temporary local file and subsequently copy it remotely.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong></dt>
<dd>File, which may be remote.</dd>
</dl>
<h2 id="yields">Yields</h2>
<p>The name of a local file which may be subsequently copied remotely.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@contextlib.contextmanager
def write_via_local_file(path: _StrOrPath) -&gt; Generator[str, None, None]:
  &#34;&#34;&#34;Context to write a temporary local file and subsequently copy it remotely.

  Args:
    path: File, which may be remote.

  Yields:
    The name of a local file which may be subsequently copied remotely.
  &#34;&#34;&#34;
  if _path_is_local(path):
    yield str(path)
  else:
    suffix = pathlib.Path(path).suffix
    with tempfile.TemporaryDirectory() as directory_name:
      tmp_path = pathlib.Path(directory_name) / f&#39;file{suffix}&#39;
      yield str(tmp_path)
      with _open(path, mode=&#39;wb&#39;) as f:
        f.write(tmp_path.read_bytes())</code></pre>
</details>
</dd>
<dt id="mediapy.write_video"><code class="name flex">
<span>def <span class="ident">write_video</span></span>(<span>path:Â Union[str,Â ForwardRef('os.PathLike[str]')], images:Â Iterable[numpy.ndarray], **kwargs:Â Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Writes images to a compressed video file.</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; video = moving_circle((480, 640), num_images=60)
&gt;&gt;&gt; write_video('/tmp/v.mp4', video, fps=60.0, qp=18)
&gt;&gt;&gt; show_video(read_video('/tmp/v.mp4'))
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong></dt>
<dd>Output video file.</dd>
<dt><strong><code>images</code></strong></dt>
<dd>Iterable over video frames, e.g. a 4D array or a list of 2D or 3D
arrays.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional parameters for <code><a title="mediapy.VideoWriter" href="#mediapy.VideoWriter">VideoWriter</a></code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_video(path: _StrOrPath, images: Iterable[np.ndarray],
                **kwargs: Any) -&gt; None:
  &#34;&#34;&#34;Writes images to a compressed video file.

  &gt;&gt;&gt; video = moving_circle((480, 640), num_images=60)
  &gt;&gt;&gt; write_video(&#39;/tmp/v.mp4&#39;, video, fps=60.0, qp=18)
  &gt;&gt;&gt; show_video(read_video(&#39;/tmp/v.mp4&#39;))

  Args:
    path: Output video file.
    images: Iterable over video frames, e.g. a 4D array or a list of 2D or 3D
      arrays.
    **kwargs: Additional parameters for `VideoWriter`.
  &#34;&#34;&#34;
  first_image, images = peek_first(images)
  shape = first_image.shape[:2]
  dtype = first_image.dtype
  if dtype == np.bool:
    dtype = np.uint8
  elif issubclass(dtype.type, np.floating):
    dtype = np.uint16
  with VideoWriter(path, shape=shape, dtype=dtype, **kwargs) as writer:
    for image in images:
      writer.add_image(image)</code></pre>
</details>
</dd>
<dt id="mediapy.ycbcr_from_rgb"><code class="name flex">
<span>def <span class="ident">ycbcr_from_rgb</span></span>(<span>rgb:Â Any) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the RGB image/video mapped to YCbCr [0,1] color space.</p>
<p>The YCbCr color space is the one called "YUV" by video compressors.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>rgb</code></strong></dt>
<dd>Input image in sRGB space.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ycbcr_from_rgb(rgb: Any) -&gt; np.ndarray:
  &#34;&#34;&#34;Returns the RGB image/video mapped to YCbCr [0,1] color space.

  The YCbCr color space is the one called &#34;YUV&#34; by video compressors.

  Args:
    rgb: Input image in sRGB space.
  &#34;&#34;&#34;
  rgb = to_float01(rgb)
  if rgb.shape[-1] != 3:
    raise ValueError(f&#39;The last dimension in {rgb.shape} is not 3.&#39;)
  return (np.matmul(rgb, _YCBCR_FROM_RGB_MATRIX) + _YCBCR_OFFSET) / 255.0</code></pre>
</details>
</dd>
<dt id="mediapy.yuv_from_rgb"><code class="name flex">
<span>def <span class="ident">yuv_from_rgb</span></span>(<span>rgb:Â Any) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the RGB image/video mapped to YUV [0,1] color space.</p>
<p>Note that the "YUV" color space used by video compressors is actually YCbCr!</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>rgb</code></strong></dt>
<dd>Input image in sRGB space.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def yuv_from_rgb(rgb: Any) -&gt; np.ndarray:
  &#34;&#34;&#34;Returns the RGB image/video mapped to YUV [0,1] color space.

  Note that the &#34;YUV&#34; color space used by video compressors is actually YCbCr!

  Args:
    rgb: Input image in sRGB space.
  &#34;&#34;&#34;
  rgb = to_float01(rgb)
  if rgb.shape[-1] != 3:
    raise ValueError(f&#39;The last dimension in {rgb.shape} is not 3.&#39;)
  return np.matmul(rgb, _YUV_FROM_RGB_MATRIX) + _YUV_CHROMA_OFFSET</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="mediapy.VideoIO"><code class="flex name class">
<span>class <span class="ident">VideoIO</span></span>
</code></dt>
<dd>
<div class="desc"><p>Base class for <code><a title="mediapy.VideoReader" href="#mediapy.VideoReader">VideoReader</a></code> and <code><a title="mediapy.VideoWriter" href="#mediapy.VideoWriter">VideoWriter</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VideoIO:
  &#34;&#34;&#34;Base class for `VideoReader` and `VideoWriter`.&#34;&#34;&#34;

  def _get_pix_fmt(self, dtype: Any, image_format: str) -&gt; str:
    &#34;&#34;&#34;Returns ffmpeg pix_fmt given data type and image format.&#34;&#34;&#34;
    native_endian_suffix = {&#39;little&#39;: &#39;le&#39;, &#39;big&#39;: &#39;be&#39;}[sys.byteorder]
    return {
        np.uint8: {
            &#39;rgb&#39;: &#39;rgb24&#39;,
            &#39;yuv&#39;: &#39;yuv444p&#39;,
            &#39;gray&#39;: &#39;gray&#39;,
        },
        np.uint16: {
            &#39;rgb&#39;: &#39;rgb48&#39; + native_endian_suffix,
            &#39;yuv&#39;: &#39;yuv444p16&#39; + native_endian_suffix,
            &#39;gray&#39;: &#39;gray16&#39; + native_endian_suffix,
        },
    }[dtype.type][image_format]</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="mediapy.VideoReader" href="#mediapy.VideoReader">VideoReader</a></li>
<li><a title="mediapy.VideoWriter" href="#mediapy.VideoWriter">VideoWriter</a></li>
</ul>
</dd>
<dt id="mediapy.VideoMetadata"><code class="flex name class">
<span>class <span class="ident">VideoMetadata</span></span>
<span>(</span><span>num_images:Â int, shape:Â Tuple[int,Â int], fps:Â float, bps:Â Union[int,Â NoneType])</span>
</code></dt>
<dd>
<div class="desc"><p>Represents the data stored in a video container header.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>num_images</code></strong></dt>
<dd>Number of frames that is expected from the video stream.
This
is estimated from the framerate and the duration stored in the video
header, so it might be inexact.</dd>
<dt><strong><code>shape</code></strong></dt>
<dd>The dimensions (height, width) of each video frame.</dd>
<dt><strong><code>fps</code></strong></dt>
<dd>The framerate in frames per second.</dd>
<dt><strong><code>bps</code></strong></dt>
<dd>The estimated bitrate of the video stream in bits per second, retrieved
from the video header.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VideoMetadata(typing.NamedTuple):
  &#34;&#34;&#34;Represents the data stored in a video container header.

  Attributes:
    num_images: Number of frames that is expected from the video stream.  This
      is estimated from the framerate and the duration stored in the video
      header, so it might be inexact.
    shape: The dimensions (height, width) of each video frame.
    fps: The framerate in frames per second.
    bps: The estimated bitrate of the video stream in bits per second, retrieved
      from the video header.
  &#34;&#34;&#34;
  num_images: int
  shape: Tuple[int, int]
  fps: float
  bps: Optional[int]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.tuple</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="mediapy.VideoMetadata.bps"><code class="name">var <span class="ident">bps</span> :Â Union[int,Â NoneType]</code></dt>
<dd>
<div class="desc"><p>Alias for field number 3</p></div>
</dd>
<dt id="mediapy.VideoMetadata.fps"><code class="name">var <span class="ident">fps</span> :Â float</code></dt>
<dd>
<div class="desc"><p>Alias for field number 2</p></div>
</dd>
<dt id="mediapy.VideoMetadata.num_images"><code class="name">var <span class="ident">num_images</span> :Â int</code></dt>
<dd>
<div class="desc"><p>Alias for field number 0</p></div>
</dd>
<dt id="mediapy.VideoMetadata.shape"><code class="name">var <span class="ident">shape</span> :Â Tuple[int,Â int]</code></dt>
<dd>
<div class="desc"><p>Alias for field number 1</p></div>
</dd>
</dl>
</dd>
<dt id="mediapy.VideoReader"><code class="flex name class">
<span>class <span class="ident">VideoReader</span></span>
<span>(</span><span>path_or_url:Â Union[str,Â ForwardRef('os.PathLike[str]')], *, output_format:Â strÂ =Â 'rgb', dtype:Â AnyÂ =Â numpy.uint8)</span>
</code></dt>
<dd>
<div class="desc"><p>Context to read a compressed video as an iterable over its images.</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; with VideoReader('/tmp/river.mp4') as reader:
...   print(f'Video has {reader.num_images} images with shape={reader.shape},'
...         f' at {reader.fps} frames/sec and {reader.bps} bits/sec.')
...   for image in reader:
...     print(image.shape)
</code></pre>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>path_or_url</code></strong></dt>
<dd>Location of input video.</dd>
<dt><strong><code>output_format</code></strong></dt>
<dd>Format of output images (default 'rgb'):
- 'rgb': Each image has shape=(height, width, 3) with R, G, B values.
- 'yuv': Each image has shape=(height, width, 3) with Y, U, V values.
- 'gray': Each image has shape=(height, width).</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>Data type for output images:
- np.uint8: Default.
- np.uint16: Allows reading 10-bit or 12-bit data without precision loss.</dd>
<dt><strong><code>metadata</code></strong></dt>
<dd>Object storing the information retrieved from the video header.
Its attributes are copied as attributes in this class.</dd>
<dt><strong><code>num_images</code></strong></dt>
<dd>Number of frames that is expected from the video stream.
This
is estimated from the framerate and the duration stored in the video
header, so it might be inexact.</dd>
<dt><strong><code>shape</code></strong></dt>
<dd>The dimensions (height, width) of each video frame.</dd>
<dt><strong><code>fps</code></strong></dt>
<dd>The framerate in frames per second.</dd>
<dt><strong><code>bps</code></strong></dt>
<dd>The estimated bitrate of the video stream in bits per second, retrieved
from the video header.</dd>
</dl>
<p>Initializes video reading from the specified path or url.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VideoReader(VideoIO, ContextManager[Any]):
  &#34;&#34;&#34;Context to read a compressed video as an iterable over its images.

  &gt;&gt;&gt; with VideoReader(&#39;/tmp/river.mp4&#39;) as reader:
  ...   print(f&#39;Video has {reader.num_images} images with shape={reader.shape},&#39;
  ...         f&#39; at {reader.fps} frames/sec and {reader.bps} bits/sec.&#39;)
  ...   for image in reader:
  ...     print(image.shape)

  Attributes:
    path_or_url: Location of input video.
    output_format: Format of output images (default &#39;rgb&#39;):
      - &#39;rgb&#39;: Each image has shape=(height, width, 3) with R, G, B values.
      - &#39;yuv&#39;: Each image has shape=(height, width, 3) with Y, U, V values.
      - &#39;gray&#39;: Each image has shape=(height, width).
    dtype: Data type for output images:
      - np.uint8: Default.
      - np.uint16: Allows reading 10-bit or 12-bit data without precision loss.
    metadata: Object storing the information retrieved from the video header.
      Its attributes are copied as attributes in this class.
    num_images: Number of frames that is expected from the video stream.  This
      is estimated from the framerate and the duration stored in the video
      header, so it might be inexact.
    shape: The dimensions (height, width) of each video frame.
    fps: The framerate in frames per second.
    bps: The estimated bitrate of the video stream in bits per second, retrieved
      from the video header.
  &#34;&#34;&#34;
  path_or_url: _StrOrPath
  output_format: str
  dtype: Any
  metadata: VideoMetadata
  num_images: int
  shape: Tuple[int, int]
  fps: float
  bps: Optional[int]
  _num_bytes_per_image: int

  def __init__(self,
               path_or_url: _StrOrPath,
               *,
               output_format: str = &#39;rgb&#39;,
               dtype: Any = np.uint8):
    &#34;&#34;&#34;Initializes video reading from the specified path or url.&#34;&#34;&#34;
    if output_format not in {&#39;rgb&#39;, &#39;yuv&#39;, &#39;gray&#39;}:
      raise ValueError(
          f&#39;Output format {output_format} is not rgb, yuv, or gray.&#39;)
    self.path_or_url = path_or_url
    self.output_format = output_format
    self.dtype = np.dtype(dtype)
    if self.dtype not in (np.uint8, np.uint16):
      raise ValueError(f&#39;Type {dtype} is not np.uint8 or np.uint16.&#39;)
    self._read_via_local_file: Any = None
    self._popen: Optional[&#39;subprocess.Popen[bytes]&#39;] = None
    self._proc: Optional[&#39;subprocess.Popen[bytes]&#39;] = None

  def __enter__(self) -&gt; &#39;VideoReader&#39;:
    ffmpeg_path = _get_ffmpeg_path()
    try:
      self._read_via_local_file = read_via_local_file(self.path_or_url)
      tmp_name = self._read_via_local_file.__enter__()

      self.metadata = _get_video_metadata(tmp_name)
      self.num_images, self.shape, self.fps, self.bps = self.metadata
      pix_fmt = self._get_pix_fmt(self.dtype, self.output_format)
      num_channels = {&#39;rgb&#39;: 3, &#39;yuv&#39;: 3, &#39;gray&#39;: 1}[self.output_format]
      bytes_per_channel = self.dtype.itemsize
      self._num_bytes_per_image = (
          np.prod(self.shape) * num_channels * bytes_per_channel)

      command = [
          ffmpeg_path, &#39;-v&#39;, &#39;panic&#39;, &#39;-nostdin&#39;, &#39;-i&#39;, tmp_name, &#39;-vcodec&#39;,
          &#39;rawvideo&#39;, &#39;-f&#39;, &#39;image2pipe&#39;, &#39;-pix_fmt&#39;, pix_fmt, &#39;-&#39;
      ]
      self._popen = subprocess.Popen(
          command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
      self._proc = self._popen.__enter__()
    except BaseException:
      self.__exit__(None, None, None)
      raise
    return self

  def __exit__(self, *_: Any) -&gt; None:
    self.close()

  def read(self) -&gt; Optional[np.ndarray]:
    &#34;&#34;&#34;Reads a video image frame (or None if at end of file).&#34;&#34;&#34;
    assert self._proc, &#39;Error: reading from an already closed context.&#39;
    assert self._proc.stdout
    data = self._proc.stdout.read(self._num_bytes_per_image)
    if not data:  # Due to either end-of-file or subprocess error.
      self.close()  # Raises exception if subprocess had error.
      return None  # To indicate end-of-file.
    assert len(data) == self._num_bytes_per_image
    image = np.frombuffer(data, dtype=self.dtype)
    if self.output_format == &#39;rgb&#39;:
      image = image.reshape(*self.shape, 3)
    elif self.output_format == &#39;yuv&#39;:  # Convert from planar YUV to pixel YUV.
      image = np.moveaxis(image.reshape(3, *self.shape), 0, 2)
    elif self.output_format == &#39;gray&#39;:  # Generate 2D rather than 3D ndimage.
      image = image.reshape(*self.shape)
    else:
      raise AssertionError
    return image

  def __iter__(self) -&gt; Generator[np.ndarray, None, None]:
    while True:
      image = self.read()
      if image is None:
        return
      yield image

  def close(self) -&gt; None:
    &#34;&#34;&#34;Terminates the piped subprocess reader.&#34;&#34;&#34;
    if self._popen:
      self._popen.__exit__(None, None, None)
      self._popen = None
      self._proc = None
    if self._read_via_local_file:
      self._read_via_local_file.__exit__(None, None, None)
      self._read_via_local_file = None</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mediapy.VideoIO" href="#mediapy.VideoIO">VideoIO</a></li>
<li>contextlib.AbstractContextManager</li>
<li>abc.ABC</li>
<li>typing.Generic</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="mediapy.VideoReader.bps"><code class="name">var <span class="ident">bps</span> :Â Union[int,Â NoneType]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="mediapy.VideoReader.dtype"><code class="name">var <span class="ident">dtype</span> :Â Any</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="mediapy.VideoReader.fps"><code class="name">var <span class="ident">fps</span> :Â float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="mediapy.VideoReader.metadata"><code class="name">var <span class="ident">metadata</span> :Â <a title="mediapy.VideoMetadata" href="#mediapy.VideoMetadata">VideoMetadata</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="mediapy.VideoReader.num_images"><code class="name">var <span class="ident">num_images</span> :Â int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="mediapy.VideoReader.output_format"><code class="name">var <span class="ident">output_format</span> :Â str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="mediapy.VideoReader.path_or_url"><code class="name">var <span class="ident">path_or_url</span> :Â Union[str,Â os.PathLike[str]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="mediapy.VideoReader.shape"><code class="name">var <span class="ident">shape</span> :Â Tuple[int,Â int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="mediapy.VideoReader.close"><code class="name flex">
<span>def <span class="ident">close</span></span>(<span>self) â€‘>Â NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Terminates the piped subprocess reader.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def close(self) -&gt; None:
  &#34;&#34;&#34;Terminates the piped subprocess reader.&#34;&#34;&#34;
  if self._popen:
    self._popen.__exit__(None, None, None)
    self._popen = None
    self._proc = None
  if self._read_via_local_file:
    self._read_via_local_file.__exit__(None, None, None)
    self._read_via_local_file = None</code></pre>
</details>
</dd>
<dt id="mediapy.VideoReader.read"><code class="name flex">
<span>def <span class="ident">read</span></span>(<span>self) â€‘>Â Union[numpy.ndarray,Â NoneType]</span>
</code></dt>
<dd>
<div class="desc"><p>Reads a video image frame (or None if at end of file).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read(self) -&gt; Optional[np.ndarray]:
  &#34;&#34;&#34;Reads a video image frame (or None if at end of file).&#34;&#34;&#34;
  assert self._proc, &#39;Error: reading from an already closed context.&#39;
  assert self._proc.stdout
  data = self._proc.stdout.read(self._num_bytes_per_image)
  if not data:  # Due to either end-of-file or subprocess error.
    self.close()  # Raises exception if subprocess had error.
    return None  # To indicate end-of-file.
  assert len(data) == self._num_bytes_per_image
  image = np.frombuffer(data, dtype=self.dtype)
  if self.output_format == &#39;rgb&#39;:
    image = image.reshape(*self.shape, 3)
  elif self.output_format == &#39;yuv&#39;:  # Convert from planar YUV to pixel YUV.
    image = np.moveaxis(image.reshape(3, *self.shape), 0, 2)
  elif self.output_format == &#39;gray&#39;:  # Generate 2D rather than 3D ndimage.
    image = image.reshape(*self.shape)
  else:
    raise AssertionError
  return image</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="mediapy.VideoWriter"><code class="flex name class">
<span>class <span class="ident">VideoWriter</span></span>
<span>(</span><span>path:Â Union[str,Â ForwardRef('os.PathLike[str]')], shape:Â Tuple[int,Â int], *, fps:Â Union[float,Â NoneType]Â =Â None, codec:Â strÂ =Â 'h264', bps:Â Union[int,Â NoneType]Â =Â None, qp:Â Union[int,Â NoneType]Â =Â None, crf:Â Union[float,Â NoneType]Â =Â None, ffmpeg_args:Â Union[str,Â Sequence[str]]Â =Â '', input_format:Â strÂ =Â 'rgb', dtype:Â AnyÂ =Â numpy.uint8, encoded_format:Â Union[str,Â NoneType]Â =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>Context to write a compressed video.</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; shape = (480, 640)
&gt;&gt;&gt; with VideoWriter('/tmp/v.mp4', shape, fps=60) as writer:
...   for image in moving_circle(shape, num_images=60):
...     writer.add_image(image)
&gt;&gt;&gt; show_video(read_video('/tmp/v.mp4'))
</code></pre>
<p>Bitrate control may be specified using at most one of: <code>bps</code>, <code>qp</code>, or <code>crf</code>.
If none are specified, <code>qp</code> is set to a default value.
See <a href="https://slhck.info/video/2017/03/01/rate-control.html">https://slhck.info/video/2017/03/01/rate-control.html</a></p>
<p>If codec is 'gif', the args <code>bps</code>, <code>qp</code>, <code>crf</code>, and <code>encoded_format</code> are
ignored.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>path</code></strong></dt>
<dd>Output video.
Its suffix (e.g. '.mp4') determines the video container
format.
The suffix must be '.gif' if the codec is 'gif'.</dd>
<dt><strong><code>shape</code></strong></dt>
<dd>2D spatial dimensions (height, width) of video image frames.
The
dimensions must be even if 'encoded_format' has subsampled chroma (e.g.,
'yuv420p' or 'yuv420p10le').</dd>
<dt><strong><code>fps</code></strong></dt>
<dd>Frames-per-second frame rate (default is 60.0 except 25.0 for 'gif').</dd>
<dt><strong><code>codec</code></strong></dt>
<dd>Compression algorithm as defined by "ffmpeg -codecs" (e.g., 'h264',
'hevc', 'vp9', or 'gif').</dd>
<dt><strong><code>bps</code></strong></dt>
<dd>Requested average bits-per-second bitrate (default None).</dd>
<dt><strong><code>qp</code></strong></dt>
<dd>Quantization parameter for video compression quality (default None).</dd>
<dt><strong><code>crf</code></strong></dt>
<dd>Constant rate factor for video compression quality (default None).</dd>
<dt><strong><code>ffmpeg_args</code></strong></dt>
<dd>Additional arguments for <code>ffmpeg</code> command, e.g. '-g 30' to
introduce I-frames, or '-bf 0' to omit B-frames.</dd>
<dt><strong><code>input_format</code></strong></dt>
<dd>Format of input images (default 'rgb'):
- 'rgb': Each image has shape=(height, width, 3) or (height, width).
- 'yuv': Each image has shape=(height, width, 3) with Y, U, V values.
- 'gray': Each image has shape=(height, width).</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>Expected data type for input images (any float input images are
converted to <code>dtype</code>):
- np.uint8: Default.
- np.uint16: Necessary when encoding &gt;8 bits/channel.</dd>
<dt><strong><code>encoded_format</code></strong></dt>
<dd>Pixel format as defined by <code>ffmpeg -pix_fmts</code>, e.g.,
'yuv420p' (2x2-subsampled chroma), 'yuv444p' (full-res chroma),
'yuv420p10le' (10-bit per channel), etc.
The default (None) selects
'yuv420p' if all shape dimensions are even, else 'yuv444p'.</dd>
</dl>
<p>Initializes video writing to the specified path.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VideoWriter(VideoIO, ContextManager[Any]):
  &#34;&#34;&#34;Context to write a compressed video.

  &gt;&gt;&gt; shape = (480, 640)
  &gt;&gt;&gt; with VideoWriter(&#39;/tmp/v.mp4&#39;, shape, fps=60) as writer:
  ...   for image in moving_circle(shape, num_images=60):
  ...     writer.add_image(image)
  &gt;&gt;&gt; show_video(read_video(&#39;/tmp/v.mp4&#39;))


  Bitrate control may be specified using at most one of: `bps`, `qp`, or `crf`.
  If none are specified, `qp` is set to a default value.
  See https://slhck.info/video/2017/03/01/rate-control.html

  If codec is &#39;gif&#39;, the args `bps`, `qp`, `crf`, and `encoded_format` are
  ignored.

  Attributes:
    path: Output video.  Its suffix (e.g. &#39;.mp4&#39;) determines the video container
      format.  The suffix must be &#39;.gif&#39; if the codec is &#39;gif&#39;.
    shape: 2D spatial dimensions (height, width) of video image frames.  The
      dimensions must be even if &#39;encoded_format&#39; has subsampled chroma (e.g.,
      &#39;yuv420p&#39; or &#39;yuv420p10le&#39;).
    fps: Frames-per-second frame rate (default is 60.0 except 25.0 for &#39;gif&#39;).
    codec: Compression algorithm as defined by &#34;ffmpeg -codecs&#34; (e.g., &#39;h264&#39;,
      &#39;hevc&#39;, &#39;vp9&#39;, or &#39;gif&#39;).
    bps: Requested average bits-per-second bitrate (default None).
    qp: Quantization parameter for video compression quality (default None).
    crf: Constant rate factor for video compression quality (default None).
    ffmpeg_args: Additional arguments for `ffmpeg` command, e.g. &#39;-g 30&#39; to
      introduce I-frames, or &#39;-bf 0&#39; to omit B-frames.
    input_format: Format of input images (default &#39;rgb&#39;):
      - &#39;rgb&#39;: Each image has shape=(height, width, 3) or (height, width).
      - &#39;yuv&#39;: Each image has shape=(height, width, 3) with Y, U, V values.
      - &#39;gray&#39;: Each image has shape=(height, width).
    dtype: Expected data type for input images (any float input images are
      converted to `dtype`):
      - np.uint8: Default.
      - np.uint16: Necessary when encoding &gt;8 bits/channel.
    encoded_format: Pixel format as defined by `ffmpeg -pix_fmts`, e.g.,
      &#39;yuv420p&#39; (2x2-subsampled chroma), &#39;yuv444p&#39; (full-res chroma),
      &#39;yuv420p10le&#39; (10-bit per channel), etc.  The default (None) selects
      &#39;yuv420p&#39; if all shape dimensions are even, else &#39;yuv444p&#39;.
  &#34;&#34;&#34;

  def __init__(self,
               path: _StrOrPath,
               shape: Tuple[int, int],
               *,
               fps: Optional[float] = None,
               codec: str = &#39;h264&#39;,
               bps: Optional[int] = None,
               qp: Optional[int] = None,
               crf: Optional[float] = None,
               ffmpeg_args: Union[str, Sequence[str]] = &#39;&#39;,
               input_format: str = &#39;rgb&#39;,
               dtype: Any = np.uint8,
               encoded_format: Optional[str] = None) -&gt; None:
    &#34;&#34;&#34;Initializes video writing to the specified path.&#34;&#34;&#34;
    _check_2d_shape(shape)
    if fps is None:
      fps = 25.0 if codec == &#39;gif&#39; else 60.0
    if fps &lt;= 0.0:
      raise ValueError(f&#39;Frame-per-second value {fps} is invalid.&#39;)
    bps = int(bps) if bps is not None else None
    if bps is not None and bps &lt;= 0:
      raise ValueError(f&#39;Bitrate value {bps} is invalid.&#39;)
    if qp is not None and (not isinstance(qp, int) or qp &lt;= 0):
      raise ValueError(
          f&#39;Quantization parameter {qp} is not a positive integer.&#39;)
    num_rate_specifications = sum(x is not None for x in (bps, qp, crf))
    if num_rate_specifications &gt; 1:
      raise ValueError(
          f&#39;Must specify at most one of bps, qp, or crf ({bps}, {qp}, {crf}).&#39;)
    ffmpeg_args = (
        shlex.split(ffmpeg_args)
        if isinstance(ffmpeg_args, str) else list(ffmpeg_args))
    if input_format not in {&#39;rgb&#39;, &#39;yuv&#39;, &#39;gray&#39;}:
      raise ValueError(f&#39;Input format {input_format} is not rgb, yuv, or gray.&#39;)
    dtype = np.dtype(dtype)
    if dtype not in (np.uint8, np.uint16):
      raise ValueError(f&#39;Type {dtype} is not np.uint8 or np.uint16.&#39;)
    self.path = pathlib.Path(path)
    self.shape = shape
    all_dimensions_are_even = all(dim % 2 == 0 for dim in shape)
    if encoded_format is None:
      encoded_format = &#39;yuv420p&#39; if all_dimensions_are_even else &#39;yuv444p&#39;
    if not all_dimensions_are_even and encoded_format.startswith(
        (&#39;yuv42&#39;, &#39;yuvj42&#39;)):
      raise ValueError(
          f&#39;With encoded_format {encoded_format}, video dimensions must be&#39;
          f&#39; even, but shape is {shape}.&#39;)
    self.fps = fps
    self.codec = codec
    self.bps = bps
    self.qp = qp
    self.crf = crf
    self.ffmpeg_args = ffmpeg_args
    self.input_format = input_format
    self.dtype = dtype
    self.encoded_format = encoded_format
    if num_rate_specifications == 0 and not ffmpeg_args:
      qp = 20 if np.prod(self.shape) &lt;= 640 * 480 else 28
    self._bitrate_args = (
        ([&#39;-vb&#39;, f&#39;{bps}&#39;] if bps is not None else []) +
        ([&#39;-qp&#39;, f&#39;{qp}&#39;] if qp is not None else []) +
        ([&#39;-vb&#39;, &#39;0&#39;, &#39;-crf&#39;, f&#39;{crf}&#39;] if crf is not None else []))
    if self.codec == &#39;gif&#39;:
      if self.path.suffix != &#39;.gif&#39;:
        raise ValueError(f&#34;File &#39;{self.path}&#39; does not have a .gif suffix.&#34;)
      self.encoded_format = &#39;rgb24&#39;
      self._bitrate_args = []
      video_filter = &#39;split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse&#39;
      self.ffmpeg_args = [&#39;-vf&#39;, video_filter, &#39;-f&#39;, &#39;gif&#39;] + self.ffmpeg_args
    self._write_via_local_file: Any = None
    self._popen: Optional[&#39;subprocess.Popen[bytes]&#39;] = None
    self._proc: Optional[&#39;subprocess.Popen[bytes]&#39;] = None

  def __enter__(self) -&gt; &#39;VideoWriter&#39;:
    ffmpeg_path = _get_ffmpeg_path()
    input_pix_fmt = self._get_pix_fmt(self.dtype, self.input_format)
    try:
      self._write_via_local_file = write_via_local_file(self.path)
      tmp_name = self._write_via_local_file.__enter__()

      # Writing to stdout using (&#39;-f&#39;, &#39;mp4&#39;, &#39;-&#39;) would require
      # (&#39;-movflags&#39;, &#39;frag_keyframe+empty_moov&#39;) and the result is nonportable.
      height, width = self.shape
      command = [
          ffmpeg_path, &#39;-v&#39;, &#39;error&#39;, &#39;-f&#39;, &#39;rawvideo&#39;, &#39;-vcodec&#39;, &#39;rawvideo&#39;,
          &#39;-pix_fmt&#39;, input_pix_fmt, &#39;-s&#39;, f&#39;{width}x{height}&#39;, &#39;-r&#39;,
          f&#39;{self.fps}&#39;, &#39;-i&#39;, &#39;-&#39;, &#39;-an&#39;, &#39;-vcodec&#39;, self.codec, &#39;-pix_fmt&#39;,
          self.encoded_format
      ] + self._bitrate_args + self.ffmpeg_args + [&#39;-y&#39;, tmp_name]
      self._popen = subprocess.Popen(
          command, stdin=subprocess.PIPE, stderr=subprocess.PIPE)
      self._proc = self._popen.__enter__()
    except BaseException:
      self.__exit__(None, None, None)
      raise
    return self

  def __exit__(self, *_: Any) -&gt; None:
    self.close()

  def add_image(self, image: np.ndarray) -&gt; None:
    &#34;&#34;&#34;Writes a video frame.

    Args:
      image: Array whose dtype and first two dimensions must match the `dtype`
        and `shape` specified in `VideoWriter` initialization.  If
        `input_format` is &#39;gray&#39;, the image must be 2D.  For the &#39;rgb&#39;
        input_format, the image may be either 2D (interpreted as grayscale) or
        3D with three (R, G, B) channels.  For the &#39;yuv&#39; input_format, the image
        must be 3D with three (Y, U, V) channels.

    Raises:
      RuntimeError: If there is an error writing to the output file.
    &#34;&#34;&#34;
    assert self._proc, &#39;Error: writing to an already closed context.&#39;
    if issubclass(image.dtype.type, (np.floating, np.bool_)):
      image = to_uint(image, self.dtype)
    if image.dtype != self.dtype:
      raise ValueError(f&#39;Image type {image.dtype} != {self.dtype}.&#39;)
    if self.input_format == &#39;gray&#39;:
      if image.ndim != 2:
        raise ValueError(f&#39;Image dimensions {image.shape} are not 2D.&#39;)
    else:
      if image.ndim == 2 and self.input_format == &#39;rgb&#39;:
        image = np.dstack((image, image, image))
      if not (image.ndim == 3 and image.shape[2] == 3):
        raise ValueError(&#39;Image dimensions {image.shape} are invalid.&#39;)
    if image.shape[:2] != self.shape:
      raise ValueError(f&#39;Image dimensions {image.shape[:2]} do not match&#39;
                       f&#39; those of the initialized video {self.shape}.&#39;)
    if self.input_format == &#39;yuv&#39;:  # Convert from per-pixel YUV to planar YUV.
      image = np.moveaxis(image, 2, 0)
    data = image.tobytes()
    assert self._proc.stdin
    if self._proc.stdin.write(data) != len(data):
      self._proc.wait()
      assert self._proc.stderr
      s = self._proc.stderr.read().decode()
      raise RuntimeError(f&#34;Error writing &#39;{self.path}&#39;: {s}&#34;)

  def close(self) -&gt; None:
    &#34;&#34;&#34;Finishes writing the video.&#34;&#34;&#34;
    if self._popen:
      assert self._proc and self._proc.stdin and self._proc.stderr
      self._proc.stdin.close()
      if self._proc.wait():
        s = self._proc.stderr.read().decode()
        raise RuntimeError(f&#34;Error writing &#39;{self.path}&#39;: {s}&#34;)
      self._popen.__exit__(None, None, None)
      self._popen = None
      self._proc = None
    if self._write_via_local_file:
      self._write_via_local_file.__exit__(None, None, None)
      self._write_via_local_file = None</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mediapy.VideoIO" href="#mediapy.VideoIO">VideoIO</a></li>
<li>contextlib.AbstractContextManager</li>
<li>abc.ABC</li>
<li>typing.Generic</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mediapy.VideoWriter.add_image"><code class="name flex">
<span>def <span class="ident">add_image</span></span>(<span>self, image:Â numpy.ndarray) â€‘>Â NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Writes a video frame.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>image</code></strong></dt>
<dd>Array whose dtype and first two dimensions must match the <code>dtype</code>
and <code>shape</code> specified in <code><a title="mediapy.VideoWriter" href="#mediapy.VideoWriter">VideoWriter</a></code> initialization.
If
<code>input_format</code> is 'gray', the image must be 2D.
For the 'rgb'
input_format, the image may be either 2D (interpreted as grayscale) or
3D with three (R, G, B) channels.
For the 'yuv' input_format, the image
must be 3D with three (Y, U, V) channels.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>RuntimeError</code></dt>
<dd>If there is an error writing to the output file.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_image(self, image: np.ndarray) -&gt; None:
  &#34;&#34;&#34;Writes a video frame.

  Args:
    image: Array whose dtype and first two dimensions must match the `dtype`
      and `shape` specified in `VideoWriter` initialization.  If
      `input_format` is &#39;gray&#39;, the image must be 2D.  For the &#39;rgb&#39;
      input_format, the image may be either 2D (interpreted as grayscale) or
      3D with three (R, G, B) channels.  For the &#39;yuv&#39; input_format, the image
      must be 3D with three (Y, U, V) channels.

  Raises:
    RuntimeError: If there is an error writing to the output file.
  &#34;&#34;&#34;
  assert self._proc, &#39;Error: writing to an already closed context.&#39;
  if issubclass(image.dtype.type, (np.floating, np.bool_)):
    image = to_uint(image, self.dtype)
  if image.dtype != self.dtype:
    raise ValueError(f&#39;Image type {image.dtype} != {self.dtype}.&#39;)
  if self.input_format == &#39;gray&#39;:
    if image.ndim != 2:
      raise ValueError(f&#39;Image dimensions {image.shape} are not 2D.&#39;)
  else:
    if image.ndim == 2 and self.input_format == &#39;rgb&#39;:
      image = np.dstack((image, image, image))
    if not (image.ndim == 3 and image.shape[2] == 3):
      raise ValueError(&#39;Image dimensions {image.shape} are invalid.&#39;)
  if image.shape[:2] != self.shape:
    raise ValueError(f&#39;Image dimensions {image.shape[:2]} do not match&#39;
                     f&#39; those of the initialized video {self.shape}.&#39;)
  if self.input_format == &#39;yuv&#39;:  # Convert from per-pixel YUV to planar YUV.
    image = np.moveaxis(image, 2, 0)
  data = image.tobytes()
  assert self._proc.stdin
  if self._proc.stdin.write(data) != len(data):
    self._proc.wait()
    assert self._proc.stderr
    s = self._proc.stderr.read().decode()
    raise RuntimeError(f&#34;Error writing &#39;{self.path}&#39;: {s}&#34;)</code></pre>
</details>
</dd>
<dt id="mediapy.VideoWriter.close"><code class="name flex">
<span>def <span class="ident">close</span></span>(<span>self) â€‘>Â NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Finishes writing the video.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def close(self) -&gt; None:
  &#34;&#34;&#34;Finishes writing the video.&#34;&#34;&#34;
  if self._popen:
    assert self._proc and self._proc.stdin and self._proc.stderr
    self._proc.stdin.close()
    if self._proc.wait():
      s = self._proc.stderr.read().decode()
      raise RuntimeError(f&#34;Error writing &#39;{self.path}&#39;: {s}&#34;)
    self._popen.__exit__(None, None, None)
    self._popen = None
    self._proc = None
  if self._write_via_local_file:
    self._write_via_local_file.__exit__(None, None, None)
    self._write_via_local_file = None</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#image-examples">Image examples</a></li>
<li><a href="#video-examples">Video examples</a></li>
</ul>
</div>
<ul id="index">
<li><h3><a href="#header-variables">Global variables</a></h3>
<ul class="">
<li><code><a title="mediapy.show_save" href="#mediapy.show_save">show_save</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="mediapy.color_ramp" href="#mediapy.color_ramp">color_ramp</a></code></li>
<li><code><a title="mediapy.compress_image" href="#mediapy.compress_image">compress_image</a></code></li>
<li><code><a title="mediapy.compress_video" href="#mediapy.compress_video">compress_video</a></code></li>
<li><code><a title="mediapy.decompress_image" href="#mediapy.decompress_image">decompress_image</a></code></li>
<li><code><a title="mediapy.decompress_video" href="#mediapy.decompress_video">decompress_video</a></code></li>
<li><code><a title="mediapy.html_from_compressed_image" href="#mediapy.html_from_compressed_image">html_from_compressed_image</a></code></li>
<li><code><a title="mediapy.html_from_compressed_video" href="#mediapy.html_from_compressed_video">html_from_compressed_video</a></code></li>
<li><code><a title="mediapy.moving_circle" href="#mediapy.moving_circle">moving_circle</a></code></li>
<li><code><a title="mediapy.peek_first" href="#mediapy.peek_first">peek_first</a></code></li>
<li><code><a title="mediapy.read_contents" href="#mediapy.read_contents">read_contents</a></code></li>
<li><code><a title="mediapy.read_image" href="#mediapy.read_image">read_image</a></code></li>
<li><code><a title="mediapy.read_via_local_file" href="#mediapy.read_via_local_file">read_via_local_file</a></code></li>
<li><code><a title="mediapy.read_video" href="#mediapy.read_video">read_video</a></code></li>
<li><code><a title="mediapy.resize_image" href="#mediapy.resize_image">resize_image</a></code></li>
<li><code><a title="mediapy.resize_video" href="#mediapy.resize_video">resize_video</a></code></li>
<li><code><a title="mediapy.rgb_from_ycbcr" href="#mediapy.rgb_from_ycbcr">rgb_from_ycbcr</a></code></li>
<li><code><a title="mediapy.rgb_from_yuv" href="#mediapy.rgb_from_yuv">rgb_from_yuv</a></code></li>
<li><code><a title="mediapy.run" href="#mediapy.run">run</a></code></li>
<li><code><a title="mediapy.set_max_output_height" href="#mediapy.set_max_output_height">set_max_output_height</a></code></li>
<li><code><a title="mediapy.set_output_height" href="#mediapy.set_output_height">set_output_height</a></code></li>
<li><code><a title="mediapy.show_image" href="#mediapy.show_image">show_image</a></code></li>
<li><code><a title="mediapy.show_images" href="#mediapy.show_images">show_images</a></code></li>
<li><code><a title="mediapy.show_video" href="#mediapy.show_video">show_video</a></code></li>
<li><code><a title="mediapy.show_videos" href="#mediapy.show_videos">show_videos</a></code></li>
<li><code><a title="mediapy.to_float01" href="#mediapy.to_float01">to_float01</a></code></li>
<li><code><a title="mediapy.to_rgb" href="#mediapy.to_rgb">to_rgb</a></code></li>
<li><code><a title="mediapy.to_type" href="#mediapy.to_type">to_type</a></code></li>
<li><code><a title="mediapy.to_uint" href="#mediapy.to_uint">to_uint</a></code></li>
<li><code><a title="mediapy.to_uint8" href="#mediapy.to_uint8">to_uint8</a></code></li>
<li><code><a title="mediapy.video_is_available" href="#mediapy.video_is_available">video_is_available</a></code></li>
<li><code><a title="mediapy.write_image" href="#mediapy.write_image">write_image</a></code></li>
<li><code><a title="mediapy.write_via_local_file" href="#mediapy.write_via_local_file">write_via_local_file</a></code></li>
<li><code><a title="mediapy.write_video" href="#mediapy.write_video">write_video</a></code></li>
<li><code><a title="mediapy.ycbcr_from_rgb" href="#mediapy.ycbcr_from_rgb">ycbcr_from_rgb</a></code></li>
<li><code><a title="mediapy.yuv_from_rgb" href="#mediapy.yuv_from_rgb">yuv_from_rgb</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="mediapy.VideoIO" href="#mediapy.VideoIO">VideoIO</a></code></h4>
</li>
<li>
<h4><code><a title="mediapy.VideoMetadata" href="#mediapy.VideoMetadata">VideoMetadata</a></code></h4>
<ul class="">
<li><code><a title="mediapy.VideoMetadata.bps" href="#mediapy.VideoMetadata.bps">bps</a></code></li>
<li><code><a title="mediapy.VideoMetadata.fps" href="#mediapy.VideoMetadata.fps">fps</a></code></li>
<li><code><a title="mediapy.VideoMetadata.num_images" href="#mediapy.VideoMetadata.num_images">num_images</a></code></li>
<li><code><a title="mediapy.VideoMetadata.shape" href="#mediapy.VideoMetadata.shape">shape</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mediapy.VideoReader" href="#mediapy.VideoReader">VideoReader</a></code></h4>
<ul class="two-column">
<li><code><a title="mediapy.VideoReader.bps" href="#mediapy.VideoReader.bps">bps</a></code></li>
<li><code><a title="mediapy.VideoReader.close" href="#mediapy.VideoReader.close">close</a></code></li>
<li><code><a title="mediapy.VideoReader.dtype" href="#mediapy.VideoReader.dtype">dtype</a></code></li>
<li><code><a title="mediapy.VideoReader.fps" href="#mediapy.VideoReader.fps">fps</a></code></li>
<li><code><a title="mediapy.VideoReader.metadata" href="#mediapy.VideoReader.metadata">metadata</a></code></li>
<li><code><a title="mediapy.VideoReader.num_images" href="#mediapy.VideoReader.num_images">num_images</a></code></li>
<li><code><a title="mediapy.VideoReader.output_format" href="#mediapy.VideoReader.output_format">output_format</a></code></li>
<li><code><a title="mediapy.VideoReader.path_or_url" href="#mediapy.VideoReader.path_or_url">path_or_url</a></code></li>
<li><code><a title="mediapy.VideoReader.read" href="#mediapy.VideoReader.read">read</a></code></li>
<li><code><a title="mediapy.VideoReader.shape" href="#mediapy.VideoReader.shape">shape</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mediapy.VideoWriter" href="#mediapy.VideoWriter">VideoWriter</a></code></h4>
<ul class="">
<li><code><a title="mediapy.VideoWriter.add_image" href="#mediapy.VideoWriter.add_image">add_image</a></code></li>
<li><code><a title="mediapy.VideoWriter.close" href="#mediapy.VideoWriter.close">close</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>